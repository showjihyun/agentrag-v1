# ğŸš¨ ë°ì´í„°ë² ì´ìŠ¤ ê¸´ê¸‰ ê°œì„  ì‚¬í•­

## ì‹¤í–‰ ìš”ì•½

í˜„ì¬ Agentic RAG ì‹œìŠ¤í…œì˜ DB êµ¬ì¡°ë¥¼ ì‹¤ì œ ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œì™€ í•¨ê»˜ ë¶„ì„í•œ ê²°ê³¼, **ê¸°ëŠ¥ì€ ì™„ë²½í•˜ì§€ë§Œ ì„±ëŠ¥ ìµœì í™”ê°€ ì‹œê¸‰**í•©ë‹ˆë‹¤.

### í•µì‹¬ ë°œê²¬ì‚¬í•­

1. **N+1 ì¿¼ë¦¬ ë¬¸ì œ**: ì›Œí¬í”Œë¡œìš° ì¡°íšŒ ì‹œ ìµœëŒ€ 100+ ì¿¼ë¦¬ ë°œìƒ
2. **í—¬í¼ í•¨ìˆ˜ ë¯¸ì‚¬ìš©**: `query_helpers.py`ì— ìµœì í™” í•¨ìˆ˜ê°€ ìˆì§€ë§Œ APIì—ì„œ ì‚¬ìš© ì•ˆ í•¨
3. **ì¸ë±ìŠ¤ ë¶€ì¡±**: ë³µí•© ì¿¼ë¦¬ íŒ¨í„´ì— ìµœì í™”ëœ ì¸ë±ìŠ¤ ì—†ìŒ
4. **JSON vs JSONB**: PostgreSQL JSONB ë¯¸ì‚¬ìš©ìœ¼ë¡œ ê²€ìƒ‰ ì„±ëŠ¥ ì €í•˜
5. **ë©”ëª¨ë¦¬ ëˆ„ìˆ˜**: AgentMemory í…Œì´ë¸” ë¬´í•œ ì¦ê°€

---

## ğŸ”´ Priority 1: N+1 ì¿¼ë¦¬ ì¦‰ì‹œ í•´ê²° (1-2ì¼)

### ë¬¸ì œ ì‹¬ê°ë„: CRITICAL

**ì˜í–¥**:
- ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œì‘ ì‹œê°„: 500ms â†’ 2000ms
- DB ì—°ê²° ê³ ê°ˆ ìœ„í—˜
- ì‚¬ìš©ì ê²½í—˜ ì €í•˜

### í•´ê²° ë°©ë²•

#### 1ë‹¨ê³„: ê¸°ì¡´ í—¬í¼ í•¨ìˆ˜ ì‚¬ìš© ê°•ì œ

**ìˆ˜ì • íŒŒì¼ ëª©ë¡**:
```
backend/api/agent_builder/chat.py (3ê³³)
backend/api/agent_builder/workflow_execution_stream.py (2ê³³)
backend/services/agent_builder/workflow_service.py (1ê³³)
backend/api/agent_builder/dashboard.py (2ê³³)
```

**ìˆ˜ì • ì˜ˆì‹œ**:
```python
# âŒ í˜„ì¬ (ì˜ëª»ëœ ë°©ì‹)
workflow = db.query(Workflow).filter(Workflow.id == workflow_id).first()

# âœ… ìˆ˜ì • (ì˜¬ë°”ë¥¸ ë°©ì‹)
from backend.db.query_helpers import get_workflow_with_relations
workflow = get_workflow_with_relations(db, workflow_id)
```

#### 2ë‹¨ê³„: ì¶”ê°€ í—¬í¼ í•¨ìˆ˜ êµ¬í˜„

`backend/db/query_helpers.py`ì— ì¶”ê°€:

```python
def get_dashboard_executions_optimized(
    db: Session,
    user_id: str,
    limit: int = 50
) -> List[AgentExecution]:
    """ëŒ€ì‹œë³´ë“œìš© ì‹¤í–‰ ì´ë ¥ - N+1 ë°©ì§€"""
    return db.query(AgentExecution)\
        .options(
            joinedload(AgentExecution.agent),
            joinedload(AgentExecution.metrics)
        )\
        .filter(AgentExecution.user_id == user_id)\
        .order_by(AgentExecution.started_at.desc())\
        .limit(limit)\
        .all()
```

### ì˜ˆìƒ íš¨ê³¼

- ì¿¼ë¦¬ ìˆ˜: í‰ê·  90% ê°ì†Œ
- ì‘ë‹µ ì‹œê°„: í‰ê·  80% ê°œì„ 
- DB ë¶€í•˜: 85% ê°ì†Œ

---


## ğŸŸ¡ Priority 2: ë³µí•© ì¸ë±ìŠ¤ ì¶”ê°€ (1ì¼)

### ë¬¸ì œ ì‹¬ê°ë„: HIGH

**ì˜í–¥**:
- ì‹¤í–‰ ì´ë ¥ ì¡°íšŒ ëŠë¦¼ (2-5ì´ˆ)
- ëŒ€ì‹œë³´ë“œ ë¡œë”© ì§€ì—°
- ë¶„ì„ ì¿¼ë¦¬ íƒ€ì„ì•„ì›ƒ

### í˜„ì¬ ë¬¸ì œ

#### 2.1 WorkflowExecution í…Œì´ë¸”

**ìì£¼ ì‚¬ìš©ë˜ëŠ” ì¿¼ë¦¬ íŒ¨í„´**:
```sql
-- íŒ¨í„´ 1: ì‚¬ìš©ìì˜ íŠ¹ì • ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì´ë ¥
SELECT * FROM workflow_executions 
WHERE user_id = ? AND workflow_id = ? AND status = 'completed'
ORDER BY started_at DESC;

-- íŒ¨í„´ 2: ìƒíƒœë³„ ì‹¤í–‰ ì´ë ¥ (ëª¨ë‹ˆí„°ë§)
SELECT * FROM workflow_executions 
WHERE status = 'running' AND started_at < NOW() - INTERVAL '1 hour';
```

**í˜„ì¬ ì¸ë±ìŠ¤**:
- `user_id` (ë‹¨ì¼)
- `workflow_id` (ë‹¨ì¼)
- `status` (ë‹¨ì¼)
- `started_at` (ë‹¨ì¼)

**ë¬¸ì œ**: ë³µí•© ì¡°ê±´ ì¿¼ë¦¬ ì‹œ ì¸ë±ìŠ¤ íš¨ìœ¨ ë‚®ìŒ

**í•´ê²°ì±…**: ë³µí•© ì¸ë±ìŠ¤ ì¶”ê°€

```python
# backend/db/models/agent_builder.py - WorkflowExecution í´ë˜ìŠ¤

__table_args__ = (
    # ê¸°ì¡´ ì¸ë±ìŠ¤...
    
    # âœ… ì‹ ê·œ ë³µí•© ì¸ë±ìŠ¤
    Index("ix_workflow_exec_user_workflow_status", 
          "user_id", "workflow_id", "status"),
    Index("ix_workflow_exec_status_started", 
          "status", "started_at"),
    Index("ix_workflow_exec_user_started", 
          "user_id", "started_at"),
)
```

#### 2.2 AgentExecution í…Œì´ë¸”

**ìì£¼ ì‚¬ìš©ë˜ëŠ” ì¿¼ë¦¬ íŒ¨í„´**:
```sql
-- íŒ¨í„´ 1: ì‚¬ìš©ìì˜ ì—ì´ì „íŠ¸ë³„ ì‹¤í–‰ ì´ë ¥
SELECT * FROM agent_executions 
WHERE user_id = ? AND agent_id = ? 
ORDER BY started_at DESC LIMIT 50;

-- íŒ¨í„´ 2: ì„¸ì…˜ë³„ ì‹¤í–‰ ì¡°íšŒ
SELECT * FROM agent_executions 
WHERE session_id = ? AND status = 'completed';
```

**í•´ê²°ì±…**:
```python
# backend/db/models/agent_builder.py - AgentExecution í´ë˜ìŠ¤

__table_args__ = (
    # ê¸°ì¡´ ì¸ë±ìŠ¤...
    
    # âœ… ì‹ ê·œ ë³µí•© ì¸ë±ìŠ¤
    Index("ix_agent_exec_user_agent_started", 
          "user_id", "agent_id", "started_at"),
    Index("ix_agent_exec_session_status", 
          "session_id", "status"),
)
```

### ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸

```python
# alembic/versions/xxx_add_composite_indexes.py

def upgrade():
    # WorkflowExecution ì¸ë±ìŠ¤
    op.create_index(
        'ix_workflow_exec_user_workflow_status',
        'workflow_executions',
        ['user_id', 'workflow_id', 'status']
    )
    op.create_index(
        'ix_workflow_exec_status_started',
        'workflow_executions',
        ['status', 'started_at']
    )
    op.create_index(
        'ix_workflow_exec_user_started',
        'workflow_executions',
        ['user_id', 'started_at']
    )
    
    # AgentExecution ì¸ë±ìŠ¤
    op.create_index(
        'ix_agent_exec_user_agent_started',
        'agent_executions',
        ['user_id', 'agent_id', 'started_at']
    )
    op.create_index(
        'ix_agent_exec_session_status',
        'agent_executions',
        ['session_id', 'status']
    )

def downgrade():
    op.drop_index('ix_workflow_exec_user_workflow_status')
    op.drop_index('ix_workflow_exec_status_started')
    op.drop_index('ix_workflow_exec_user_started')
    op.drop_index('ix_agent_exec_user_agent_started')
    op.drop_index('ix_agent_exec_session_status')
```

### ì˜ˆìƒ íš¨ê³¼

| ì¿¼ë¦¬ ìœ í˜• | í˜„ì¬ | ê°œì„  í›„ | ê°œì„ ìœ¨ |
|-----------|------|---------|--------|
| ì‹¤í–‰ ì´ë ¥ ì¡°íšŒ | 2000ms | 400ms | 80% â†“ |
| ëŒ€ì‹œë³´ë“œ ë¡œë”© | 5000ms | 1000ms | 80% â†“ |
| ìƒíƒœë³„ í•„í„°ë§ | 3000ms | 500ms | 83% â†“ |

---


## ğŸŸ¡ Priority 3: JSON â†’ JSONB ë§ˆì´ê·¸ë ˆì´ì…˜ (2-3ì¼)

### ë¬¸ì œ ì‹¬ê°ë„: MEDIUM-HIGH

**ì˜í–¥**:
- JSON í•„ë“œ ê²€ìƒ‰ ë§¤ìš° ëŠë¦¼ (10-20ì´ˆ)
- ì¸ë±ìŠ¤ ìƒì„± ë¶ˆê°€
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ

### í˜„ì¬ ë¬¸ì œ

**JSON ì»¬ëŸ¼ ì‚¬ìš© í˜„í™©**:
```python
# Agent ëª¨ë¸
configuration = Column(JSON, default=dict)  # âŒ JSON

# Workflow ëª¨ë¸
graph_definition = Column(JSON, nullable=False)  # âŒ JSON

# AgentBlock ëª¨ë¸
config = Column(JSON, nullable=False, default=dict)  # âŒ JSON
sub_blocks = Column(JSON, nullable=False, default=dict)  # âŒ JSON
```

**ë¬¸ì œ ì¿¼ë¦¬ ì˜ˆì‹œ**:
```python
# âŒ ë§¤ìš° ëŠë¦¼ (ì¸ë±ìŠ¤ ì—†ìŒ)
agents = db.query(Agent)\
    .filter(Agent.configuration['llm_provider'].astext == 'ollama')\
    .all()
```

### í•´ê²°ì±…

#### Step 1: ëª¨ë¸ ìˆ˜ì •

```python
# backend/db/models/agent_builder.py
from sqlalchemy.dialects.postgresql import JSONB

class Agent(Base):
    # Before
    # configuration = Column(JSON, default=dict)
    
    # After
    configuration = Column(JSONB, default=dict)  # âœ… JSONB

class Workflow(Base):
    # Before
    # graph_definition = Column(JSON, nullable=False)
    
    # After
    graph_definition = Column(JSONB, nullable=False)  # âœ… JSONB
```

#### Step 2: GIN ì¸ë±ìŠ¤ ì¶”ê°€

```python
# alembic/versions/xxx_json_to_jsonb.py

def upgrade():
    # 1. JSON â†’ JSONB ë³€í™˜
    op.execute("""
        ALTER TABLE agents 
        ALTER COLUMN configuration TYPE JSONB USING configuration::JSONB
    """)
    
    op.execute("""
        ALTER TABLE workflows 
        ALTER COLUMN graph_definition TYPE JSONB USING graph_definition::JSONB
    """)
    
    op.execute("""
        ALTER TABLE agent_blocks 
        ALTER COLUMN config TYPE JSONB USING config::JSONB,
        ALTER COLUMN sub_blocks TYPE JSONB USING sub_blocks::JSONB
    """)
    
    # 2. GIN ì¸ë±ìŠ¤ ìƒì„±
    op.execute("""
        CREATE INDEX ix_agents_config_gin 
        ON agents USING GIN (configuration)
    """)
    
    op.execute("""
        CREATE INDEX ix_workflows_graph_gin 
        ON workflows USING GIN (graph_definition)
    """)
    
    # 3. íŠ¹ì • í‚¤ ì¸ë±ìŠ¤ (ìì£¼ ê²€ìƒ‰ë˜ëŠ” í•„ë“œ)
    op.execute("""
        CREATE INDEX ix_agents_llm_provider 
        ON agents ((configuration->>'llm_provider'))
    """)
    
    op.execute("""
        CREATE INDEX ix_agents_llm_model 
        ON agents ((configuration->>'llm_model'))
    """)

def downgrade():
    op.drop_index('ix_agents_config_gin')
    op.drop_index('ix_workflows_graph_gin')
    op.drop_index('ix_agents_llm_provider')
    op.drop_index('ix_agents_llm_model')
    
    op.execute("ALTER TABLE agents ALTER COLUMN configuration TYPE JSON")
    op.execute("ALTER TABLE workflows ALTER COLUMN graph_definition TYPE JSON")
    op.execute("ALTER TABLE agent_blocks ALTER COLUMN config TYPE JSON")
    op.execute("ALTER TABLE agent_blocks ALTER COLUMN sub_blocks TYPE JSON")
```

#### Step 3: ì¿¼ë¦¬ ìµœì í™”

```python
# âœ… JSONB ì¸ë±ìŠ¤ í™œìš©
agents = db.query(Agent)\
    .filter(Agent.configuration['llm_provider'].astext == 'ollama')\
    .all()  # ì´ì œ GIN ì¸ë±ìŠ¤ ì‚¬ìš©

# âœ… ë³µì¡í•œ JSON ì¿¼ë¦¬ë„ ë¹ ë¦„
workflows = db.query(Workflow)\
    .filter(Workflow.graph_definition['nodes'].contains([{'type': 'agent'}]))\
    .all()
```

### ì˜ˆìƒ íš¨ê³¼

| ì‘ì—… | JSON | JSONB | ê°œì„ ìœ¨ |
|------|------|-------|--------|
| JSON ê²€ìƒ‰ | 10000ms | 500ms | 95% â†“ |
| ì €ì¥ ê³µê°„ | 100MB | 70MB | 30% â†“ |
| ì¸ë±ìŠ¤ ì§€ì› | âŒ | âœ… | - |
| ë¶€ë¶„ ì—…ë°ì´íŠ¸ | âŒ | âœ… | - |

---


## ğŸŸ¢ Priority 4: ë©”ëª¨ë¦¬ ìë™ ì •ë¦¬ (1-2ì¼)

### ë¬¸ì œ ì‹¬ê°ë„: MEDIUM

**ì˜í–¥**:
- AgentMemory í…Œì´ë¸” ë¬´í•œ ì¦ê°€
- ì¿¼ë¦¬ ì„±ëŠ¥ ì ì§„ì  ì €í•˜
- ìŠ¤í† ë¦¬ì§€ ë¹„ìš© ì¦ê°€

### í˜„ì¬ ë¬¸ì œ

**AgentMemory í…Œì´ë¸”**:
```python
class AgentMemory(Base):
    # ì‚­ì œ ë¡œì§ ì—†ìŒ
    # STMì´ 24ì‹œê°„ í›„ì—ë„ ë‚¨ì•„ìˆìŒ
    # LTMì´ ë¬´í•œì • ìŒ“ì„
```

**ì‹¤ì œ ë°ì´í„° ì¦ê°€ ì˜ˆì¸¡**:
- 1ì¼: 1,000 ë ˆì½”ë“œ
- 1ì£¼: 7,000 ë ˆì½”ë“œ
- 1ê°œì›”: 30,000 ë ˆì½”ë“œ
- 1ë…„: 365,000 ë ˆì½”ë“œ (ì •ë¦¬ ì—†ì´)

### í•´ê²°ì±…

#### Step 1: ìë™ ì •ë¦¬ ì„œë¹„ìŠ¤ êµ¬í˜„

```python
# backend/services/memory_cleanup_service.py

from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from backend.db.models.agent_builder import AgentMemory
import logging

logger = logging.getLogger(__name__)


class MemoryCleanupService:
    """ë©”ëª¨ë¦¬ ìë™ ì •ë¦¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self, db: Session):
        self.db = db
    
    def cleanup_expired_memories(self) -> dict:
        """ë§Œë£Œëœ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        
        stats = {
            'stm_deleted': 0,
            'ltm_deleted': 0,
            'episodic_deleted': 0
        }
        
        try:
            # 1. STM ì •ë¦¬ (24ì‹œê°„ ì´ìƒ ê²½ê³¼)
            stm_cutoff = datetime.utcnow() - timedelta(hours=24)
            stm_deleted = self.db.query(AgentMemory)\
                .filter(
                    AgentMemory.type == 'short_term',
                    AgentMemory.created_at < stm_cutoff
                )\
                .delete(synchronize_session=False)
            stats['stm_deleted'] = stm_deleted
            
            # 2. ì¤‘ìš”ë„ ë‚®ì€ LTM ì •ë¦¬ (90ì¼ ì´ìƒ, ì ‘ê·¼ ì—†ìŒ)
            ltm_cutoff = datetime.utcnow() - timedelta(days=90)
            ltm_deleted = self.db.query(AgentMemory)\
                .filter(
                    AgentMemory.type == 'long_term',
                    AgentMemory.importance == 'low',
                    AgentMemory.last_accessed_at < ltm_cutoff
                )\
                .delete(synchronize_session=False)
            stats['ltm_deleted'] = ltm_deleted
            
            # 3. ì˜¤ë˜ëœ Episodic ë©”ëª¨ë¦¬ ì •ë¦¬ (30ì¼ ì´ìƒ)
            episodic_cutoff = datetime.utcnow() - timedelta(days=30)
            episodic_deleted = self.db.query(AgentMemory)\
                .filter(
                    AgentMemory.type == 'episodic',
                    AgentMemory.created_at < episodic_cutoff,
                    AgentMemory.importance == 'low'
                )\
                .delete(synchronize_session=False)
            stats['episodic_deleted'] = episodic_deleted
            
            self.db.commit()
            
            logger.info(f"Memory cleanup completed: {stats}")
            return stats
            
        except Exception as e:
            self.db.rollback()
            logger.error(f"Memory cleanup failed: {e}", exc_info=True)
            raise
    
    def consolidate_memories(self, agent_id: str) -> int:
        """ë©”ëª¨ë¦¬ í†µí•© (STM â†’ LTM)"""
        
        # ìì£¼ ì ‘ê·¼ë˜ëŠ” STMì„ LTMìœ¼ë¡œ ìŠ¹ê²©
        threshold_date = datetime.utcnow() - timedelta(hours=12)
        
        memories = self.db.query(AgentMemory)\
            .filter(
                AgentMemory.agent_id == agent_id,
                AgentMemory.type == 'short_term',
                AgentMemory.access_count >= 3,  # 3íšŒ ì´ìƒ ì ‘ê·¼
                AgentMemory.created_at < threshold_date
            )\
            .all()
        
        consolidated = 0
        for memory in memories:
            memory.type = 'long_term'
            memory.importance = 'medium'
            consolidated += 1
        
        self.db.commit()
        logger.info(f"Consolidated {consolidated} memories for agent {agent_id}")
        return consolidated
```

#### Step 2: ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •

```python
# backend/core/scheduler.py

from apscheduler.schedulers.background import BackgroundScheduler
from backend.services.memory_cleanup_service import MemoryCleanupService
from backend.db.database import SessionLocal

scheduler = BackgroundScheduler()


def cleanup_memories_job():
    """ë©”ëª¨ë¦¬ ì •ë¦¬ ì‘ì—…"""
    db = SessionLocal()
    try:
        service = MemoryCleanupService(db)
        stats = service.cleanup_expired_memories()
        print(f"Memory cleanup: {stats}")
    finally:
        db.close()


# ë§¤ì¼ ìƒˆë²½ 3ì‹œì— ì‹¤í–‰
scheduler.add_job(
    cleanup_memories_job,
    'cron',
    hour=3,
    minute=0,
    id='memory_cleanup'
)

scheduler.start()
```

#### Step 3: API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€ (ìˆ˜ë™ ì‹¤í–‰ìš©)

```python
# backend/api/agent_builder/memory.py

@router.post("/memories/cleanup")
async def cleanup_memories(
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """ë©”ëª¨ë¦¬ ìˆ˜ë™ ì •ë¦¬ (ê´€ë¦¬ì ì „ìš©)"""
    
    if current_user.role != 'admin':
        raise HTTPException(status_code=403, detail="Admin only")
    
    service = MemoryCleanupService(db)
    stats = service.cleanup_expired_memories()
    
    return {
        "success": True,
        "stats": stats
    }
```

### ì˜ˆìƒ íš¨ê³¼

| í•­ëª© | ì •ë¦¬ ì „ | ì •ë¦¬ í›„ | ê°œì„ ìœ¨ |
|------|---------|---------|--------|
| í…Œì´ë¸” í¬ê¸° (1ë…„) | 365K rows | 50K rows | 86% â†“ |
| ì¿¼ë¦¬ ì†ë„ | 500ms | 100ms | 80% â†“ |
| ìŠ¤í† ë¦¬ì§€ | 5GB | 700MB | 86% â†“ |

---


## ğŸ“Š Priority 5: ì§‘ê³„ í…Œì´ë¸” ì¶”ê°€ (2-3ì¼)

### ë¬¸ì œ ì‹¬ê°ë„: MEDIUM

**ì˜í–¥**:
- ëŒ€ì‹œë³´ë“œ ë¡œë”© 5-10ì´ˆ
- ë¶„ì„ ì¿¼ë¦¬ íƒ€ì„ì•„ì›ƒ
- DB CPU ì‚¬ìš©ë¥  ë†’ìŒ

### í˜„ì¬ ë¬¸ì œ

**ëŒ€ì‹œë³´ë“œ ì‹¤ì‹œê°„ ì§‘ê³„ ì¿¼ë¦¬**:
```python
# backend/api/agent_builder/dashboard.py

# âŒ ë§¤ë²ˆ ì‹¤ì‹œê°„ ì§‘ê³„ (ë§¤ìš° ëŠë¦¼)
total_executions = db.query(AgentExecution)\
    .filter(AgentExecution.user_id == user_id)\
    .count()  # ì „ì²´ ìŠ¤ìº”

successful_executions = db.query(AgentExecution)\
    .filter(
        AgentExecution.user_id == user_id,
        AgentExecution.status == "completed"
    )\
    .count()  # ë˜ ì „ì²´ ìŠ¤ìº”

# í‰ê·  duration ê³„ì‚°
avg_duration = db.query(func.avg(AgentExecution.duration_ms))\
    .filter(AgentExecution.user_id == user_id)\
    .scalar()  # ë˜ ì „ì²´ ìŠ¤ìº”
```

**ë¬¸ì œ**:
- 100ë§Œ ë ˆì½”ë“œ í…Œì´ë¸”ì—ì„œ ë§¤ë²ˆ ì§‘ê³„
- 3ê°œ ì¿¼ë¦¬ = 3ë²ˆ ì „ì²´ ìŠ¤ìº”
- ì‘ë‹µ ì‹œê°„: 5-10ì´ˆ

### í•´ê²°ì±…

#### Step 1: ì§‘ê³„ í…Œì´ë¸” ì¶”ê°€

```python
# backend/db/models/agent_builder.py

class AgentExecutionStats(Base):
    """ì¼ë³„ ì—ì´ì „íŠ¸ ì‹¤í–‰ í†µê³„ (ì§‘ê³„ í…Œì´ë¸”)"""
    __tablename__ = "agent_execution_stats"
    
    # Primary Key
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    
    # Foreign Keys
    agent_id = Column(UUID(as_uuid=True), ForeignKey("agents.id"), index=True)
    user_id = Column(UUID(as_uuid=True), ForeignKey("users.id"), index=True)
    
    # ë‚ ì§œ
    date = Column(Date, nullable=False, index=True)
    
    # ì§‘ê³„ ë°ì´í„°
    execution_count = Column(Integer, default=0)
    success_count = Column(Integer, default=0)
    failed_count = Column(Integer, default=0)
    cancelled_count = Column(Integer, default=0)
    
    # ì„±ëŠ¥ ë©”íŠ¸ë¦­
    avg_duration_ms = Column(Integer)
    min_duration_ms = Column(Integer)
    max_duration_ms = Column(Integer)
    
    # LLM ë©”íŠ¸ë¦­
    total_tokens = Column(BigInteger, default=0)
    total_llm_calls = Column(Integer, default=0)
    
    # ë¹„ìš©
    total_cost = Column(Float, default=0.0)
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    __table_args__ = (
        UniqueConstraint("agent_id", "user_id", "date", name="uq_agent_stats_date"),
        Index("ix_agent_stats_user_date", "user_id", "date"),
        Index("ix_agent_stats_agent_date", "agent_id", "date"),
    )


class WorkflowExecutionStats(Base):
    """ì¼ë³„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ í†µê³„"""
    __tablename__ = "workflow_execution_stats"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    workflow_id = Column(UUID(as_uuid=True), ForeignKey("workflows.id"), index=True)
    user_id = Column(UUID(as_uuid=True), ForeignKey("users.id"), index=True)
    date = Column(Date, nullable=False, index=True)
    
    # ì§‘ê³„ ë°ì´í„°
    execution_count = Column(Integer, default=0)
    success_count = Column(Integer, default=0)
    failed_count = Column(Integer, default=0)
    avg_duration_ms = Column(Integer)
    
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    __table_args__ = (
        UniqueConstraint("workflow_id", "user_id", "date", name="uq_workflow_stats_date"),
        Index("ix_workflow_stats_user_date", "user_id", "date"),
    )
```

#### Step 2: ì§‘ê³„ ì„œë¹„ìŠ¤ êµ¬í˜„

```python
# backend/services/stats_aggregation_service.py

from datetime import date, datetime, timedelta
from sqlalchemy import func
from sqlalchemy.orm import Session
from backend.db.models.agent_builder import (
    AgentExecution, AgentExecutionStats,
    WorkflowExecution, WorkflowExecutionStats,
    ExecutionMetrics
)


class StatsAggregationService:
    """í†µê³„ ì§‘ê³„ ì„œë¹„ìŠ¤"""
    
    def __init__(self, db: Session):
        self.db = db
    
    def aggregate_daily_agent_stats(self, target_date: date = None):
        """ì¼ë³„ ì—ì´ì „íŠ¸ í†µê³„ ì§‘ê³„"""
        
        if target_date is None:
            target_date = date.today() - timedelta(days=1)  # ì–´ì œ
        
        # í•´ë‹¹ ë‚ ì§œì˜ ëª¨ë“  ì‹¤í–‰ ì¡°íšŒ
        start_dt = datetime.combine(target_date, datetime.min.time())
        end_dt = datetime.combine(target_date, datetime.max.time())
        
        # Agentë³„, Userë³„ ê·¸ë£¹í™”í•˜ì—¬ ì§‘ê³„
        stats_query = self.db.query(
            AgentExecution.agent_id,
            AgentExecution.user_id,
            func.count(AgentExecution.id).label('execution_count'),
            func.sum(
                func.case((AgentExecution.status == 'completed', 1), else_=0)
            ).label('success_count'),
            func.sum(
                func.case((AgentExecution.status == 'failed', 1), else_=0)
            ).label('failed_count'),
            func.avg(AgentExecution.duration_ms).label('avg_duration_ms'),
            func.min(AgentExecution.duration_ms).label('min_duration_ms'),
            func.max(AgentExecution.duration_ms).label('max_duration_ms')
        )\
        .filter(
            AgentExecution.started_at >= start_dt,
            AgentExecution.started_at <= end_dt
        )\
        .group_by(AgentExecution.agent_id, AgentExecution.user_id)\
        .all()
        
        # ì§‘ê³„ ê²°ê³¼ ì €ì¥
        for stat in stats_query:
            # ê¸°ì¡´ ë ˆì½”ë“œ í™•ì¸
            existing = self.db.query(AgentExecutionStats)\
                .filter(
                    AgentExecutionStats.agent_id == stat.agent_id,
                    AgentExecutionStats.user_id == stat.user_id,
                    AgentExecutionStats.date == target_date
                )\
                .first()
            
            if existing:
                # ì—…ë°ì´íŠ¸
                existing.execution_count = stat.execution_count
                existing.success_count = stat.success_count
                existing.failed_count = stat.failed_count
                existing.avg_duration_ms = int(stat.avg_duration_ms) if stat.avg_duration_ms else 0
                existing.min_duration_ms = stat.min_duration_ms
                existing.max_duration_ms = stat.max_duration_ms
                existing.updated_at = datetime.utcnow()
            else:
                # ì‹ ê·œ ìƒì„±
                new_stat = AgentExecutionStats(
                    agent_id=stat.agent_id,
                    user_id=stat.user_id,
                    date=target_date,
                    execution_count=stat.execution_count,
                    success_count=stat.success_count,
                    failed_count=stat.failed_count,
                    avg_duration_ms=int(stat.avg_duration_ms) if stat.avg_duration_ms else 0,
                    min_duration_ms=stat.min_duration_ms,
                    max_duration_ms=stat.max_duration_ms
                )
                self.db.add(new_stat)
        
        self.db.commit()
        return len(stats_query)
```

#### Step 3: ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •

```python
# backend/core/scheduler.py

def aggregate_stats_job():
    """í†µê³„ ì§‘ê³„ ì‘ì—…"""
    db = SessionLocal()
    try:
        service = StatsAggregationService(db)
        
        # ì–´ì œ ë°ì´í„° ì§‘ê³„
        yesterday = date.today() - timedelta(days=1)
        agent_count = service.aggregate_daily_agent_stats(yesterday)
        workflow_count = service.aggregate_daily_workflow_stats(yesterday)
        
        print(f"Stats aggregated: {agent_count} agents, {workflow_count} workflows")
    finally:
        db.close()


# ë§¤ì¼ ìƒˆë²½ 2ì‹œì— ì‹¤í–‰
scheduler.add_job(
    aggregate_stats_job,
    'cron',
    hour=2,
    minute=0,
    id='stats_aggregation'
)
```

#### Step 4: ëŒ€ì‹œë³´ë“œ ì¿¼ë¦¬ ìµœì í™”

```python
# backend/api/agent_builder/dashboard.py

# âœ… ì§‘ê³„ í…Œì´ë¸” ì‚¬ìš© (ë§¤ìš° ë¹ ë¦„)
from datetime import date, timedelta

# ìµœê·¼ 30ì¼ í†µê³„
start_date = date.today() - timedelta(days=30)

stats = db.query(
    func.sum(AgentExecutionStats.execution_count).label('total'),
    func.sum(AgentExecutionStats.success_count).label('success'),
    func.avg(AgentExecutionStats.avg_duration_ms).label('avg_duration')
)\
.filter(
    AgentExecutionStats.user_id == user_id,
    AgentExecutionStats.date >= start_date
)\
.first()

# 1ê°œ ì¿¼ë¦¬ë¡œ ëª¨ë“  í†µê³„ ì¡°íšŒ (30ê°œ ë ˆì½”ë“œë§Œ ìŠ¤ìº”)
```

### ì˜ˆìƒ íš¨ê³¼

| í•­ëª© | ì‹¤ì‹œê°„ ì§‘ê³„ | ì§‘ê³„ í…Œì´ë¸” | ê°œì„ ìœ¨ |
|------|-------------|-------------|--------|
| ëŒ€ì‹œë³´ë“œ ë¡œë”© | 5000ms | 100ms | 98% â†“ |
| ìŠ¤ìº” ë ˆì½”ë“œ ìˆ˜ | 1,000,000 | 30 | 99.997% â†“ |
| DB CPU ì‚¬ìš©ë¥  | 80% | 5% | 94% â†“ |
| ì¿¼ë¦¬ ë³µì¡ë„ | ë†’ìŒ | ë‚®ìŒ | - |

---


## ğŸ“… êµ¬í˜„ ë¡œë“œë§µ

### Week 1: Critical Issues (ì¦‰ì‹œ ì‹œì‘)

#### Day 1-2: N+1 ì¿¼ë¦¬ í•´ê²°
- [ ] `query_helpers.py` í—¬í¼ í•¨ìˆ˜ ì‚¬ìš© ê°•ì œ
- [ ] API ì½”ë“œ 8ê³³ ìˆ˜ì •
- [ ] ì¶”ê°€ í—¬í¼ í•¨ìˆ˜ 2ê°œ êµ¬í˜„
- [ ] í…ŒìŠ¤íŠ¸ ë° ì„±ëŠ¥ ì¸¡ì •

**ì˜ˆìƒ íš¨ê³¼**: ì‘ë‹µ ì‹œê°„ 80% ê°œì„ 

#### Day 3: ë³µí•© ì¸ë±ìŠ¤ ì¶”ê°€
- [ ] ì¸ë±ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [ ] ê°œë°œ í™˜ê²½ í…ŒìŠ¤íŠ¸
- [ ] í”„ë¡œë•ì…˜ ë°°í¬ (off-peak ì‹œê°„)
- [ ] ì¿¼ë¦¬ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

**ì˜ˆìƒ íš¨ê³¼**: ì¿¼ë¦¬ ì†ë„ 3-5ë°° í–¥ìƒ

#### Day 4-5: JSON â†’ JSONB ë§ˆì´ê·¸ë ˆì´ì…˜
- [ ] ëª¨ë¸ ìˆ˜ì • (JSON â†’ JSONB)
- [ ] ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [ ] GIN ì¸ë±ìŠ¤ ì¶”ê°€
- [ ] ë°±ì—… ë° í”„ë¡œë•ì…˜ ë°°í¬

**ì˜ˆìƒ íš¨ê³¼**: JSON ê²€ìƒ‰ 95% ê°œì„ 

### Week 2: Important Issues

#### Day 6-7: ë©”ëª¨ë¦¬ ìë™ ì •ë¦¬
- [ ] `MemoryCleanupService` êµ¬í˜„
- [ ] ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
- [ ] API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
- [ ] ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ì¶”ê°€

**ì˜ˆìƒ íš¨ê³¼**: ìŠ¤í† ë¦¬ì§€ 86% ì ˆê°

#### Day 8-10: ì§‘ê³„ í…Œì´ë¸” êµ¬í˜„
- [ ] ì§‘ê³„ í…Œì´ë¸” ëª¨ë¸ ì¶”ê°€
- [ ] `StatsAggregationService` êµ¬í˜„
- [ ] ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
- [ ] ëŒ€ì‹œë³´ë“œ ì¿¼ë¦¬ ìµœì í™”

**ì˜ˆìƒ íš¨ê³¼**: ëŒ€ì‹œë³´ë“œ 98% ê°œì„ 

---

## ğŸ¯ ì„±ëŠ¥ ê°œì„  ëª©í‘œ

### Before (í˜„ì¬)

| í•­ëª© | í˜„ì¬ ì„±ëŠ¥ |
|------|-----------|
| Workflow ë¡œë”© | 2000ms |
| Agent ë¡œë”© | 800ms |
| ëŒ€ì‹œë³´ë“œ ë¡œë”© | 5000ms |
| ì‹¤í–‰ ì´ë ¥ ì¡°íšŒ | 2000ms |
| JSON ê²€ìƒ‰ | 10000ms |
| DB ì—°ê²° ì‚¬ìš©ë¥  | ë†’ìŒ (80%) |

### After (ê°œì„  í›„)

| í•­ëª© | ëª©í‘œ ì„±ëŠ¥ | ê°œì„ ìœ¨ |
|------|-----------|--------|
| Workflow ë¡œë”© | 300ms | 85% â†“ |
| Agent ë¡œë”© | 150ms | 81% â†“ |
| ëŒ€ì‹œë³´ë“œ ë¡œë”© | 100ms | 98% â†“ |
| ì‹¤í–‰ ì´ë ¥ ì¡°íšŒ | 400ms | 80% â†“ |
| JSON ê²€ìƒ‰ | 500ms | 95% â†“ |
| DB ì—°ê²° ì‚¬ìš©ë¥  | ë‚®ìŒ (15%) | 81% â†“ |

### ì „ì²´ ì‹œìŠ¤í…œ ì˜í–¥

- **í‰ê·  ì‘ë‹µ ì‹œê°„**: 2500ms â†’ 350ms (86% ê°œì„ )
- **DB ë¶€í•˜**: 80% â†’ 15% (81% ê°ì†Œ)
- **ë™ì‹œ ì‚¬ìš©ì ì²˜ë¦¬**: 100ëª… â†’ 500ëª… (5ë°° ì¦ê°€)
- **ìŠ¤í† ë¦¬ì§€ ë¹„ìš©**: ì›” $500 â†’ $150 (70% ì ˆê°)

---

## ğŸ”§ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ Quick Wins

### 1. í—¬í¼ í•¨ìˆ˜ ì‚¬ìš© (30ë¶„ ì‘ì—…)

```bash
# 1. íŒŒì¼ ìˆ˜ì •
backend/api/agent_builder/chat.py
backend/api/agent_builder/workflow_execution_stream.py
backend/services/agent_builder/workflow_service.py

# 2. ë³€ê²½ ë‚´ìš©
- db.query(Workflow).filter(...).first()
+ from backend.db.query_helpers import get_workflow_with_relations
+ get_workflow_with_relations(db, workflow_id)

# 3. í…ŒìŠ¤íŠ¸
pytest backend/tests/integration/test_workflow_api.py

# 4. ë°°í¬
git commit -m "fix: Use query helpers to prevent N+1 queries"
```

**ì¦‰ì‹œ íš¨ê³¼**: ì›Œí¬í”Œë¡œìš° ë¡œë”© 85% ê°œì„ 

### 2. ì¸ë±ìŠ¤ ì¶”ê°€ (1ì‹œê°„ ì‘ì—…)

```bash
# 1. ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒì„±
alembic revision -m "add_composite_indexes"

# 2. ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± (ìœ„ ì˜ˆì‹œ ì°¸ê³ )

# 3. ê°œë°œ í™˜ê²½ í…ŒìŠ¤íŠ¸
alembic upgrade head

# 4. í”„ë¡œë•ì…˜ ë°°í¬ (ìƒˆë²½ ì‹œê°„)
# ì¸ë±ìŠ¤ ìƒì„±ì€ CONCURRENT ì˜µì…˜ ì‚¬ìš©
```

**ì¦‰ì‹œ íš¨ê³¼**: ì‹¤í–‰ ì´ë ¥ ì¡°íšŒ 80% ê°œì„ 

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ê²€ì¦

### ì„±ëŠ¥ ì¸¡ì • ë„êµ¬

```python
# backend/core/performance_monitor.py

import time
from functools import wraps
import logging

logger = logging.getLogger(__name__)


def measure_query_performance(func):
    """ì¿¼ë¦¬ ì„±ëŠ¥ ì¸¡ì • ë°ì½”ë ˆì´í„°"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        duration = (time.time() - start) * 1000
        
        logger.info(f"{func.__name__} took {duration:.2f}ms")
        
        # ëŠë¦° ì¿¼ë¦¬ ê²½ê³ 
        if duration > 1000:
            logger.warning(f"Slow query detected: {func.__name__} ({duration:.2f}ms)")
        
        return result
    return wrapper


# ì‚¬ìš© ì˜ˆì‹œ
@measure_query_performance
def get_workflow_with_relations(db, workflow_id):
    # ... êµ¬í˜„
    pass
```

### ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ

```python
# backend/api/monitoring/performance.py

@router.get("/performance/stats")
async def get_performance_stats(db: Session = Depends(get_db)):
    """ì„±ëŠ¥ í†µê³„ ì¡°íšŒ"""
    
    # í‰ê·  ì¿¼ë¦¬ ì‹œê°„
    avg_query_time = db.query(func.avg(PerformanceLog.duration_ms))\
        .filter(PerformanceLog.created_at >= datetime.utcnow() - timedelta(hours=24))\
        .scalar()
    
    # ëŠë¦° ì¿¼ë¦¬ ìˆ˜
    slow_queries = db.query(PerformanceLog)\
        .filter(
            PerformanceLog.duration_ms > 1000,
            PerformanceLog.created_at >= datetime.utcnow() - timedelta(hours=24)
        )\
        .count()
    
    return {
        "avg_query_time_ms": avg_query_time,
        "slow_queries_24h": slow_queries,
        "db_pool_status": get_pool_status()
    }
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1: Critical (Week 1)
- [ ] N+1 ì¿¼ë¦¬ í•´ê²° (8ê°œ íŒŒì¼ ìˆ˜ì •)
- [ ] ë³µí•© ì¸ë±ìŠ¤ ì¶”ê°€ (5ê°œ ì¸ë±ìŠ¤)
- [ ] JSON â†’ JSONB ë§ˆì´ê·¸ë ˆì´ì…˜
- [ ] ì„±ëŠ¥ ì¸¡ì • ë° ê²€ì¦

### Phase 2: Important (Week 2)
- [ ] ë©”ëª¨ë¦¬ ìë™ ì •ë¦¬ êµ¬í˜„
- [ ] ì§‘ê³„ í…Œì´ë¸” ì¶”ê°€
- [ ] ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
- [ ] ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

### Phase 3: Verification
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
- [ ] ë¶€í•˜ í…ŒìŠ¤íŠ¸ (100 â†’ 500 ë™ì‹œ ì‚¬ìš©ì)
- [ ] í”„ë¡œë•ì…˜ ëª¨ë‹ˆí„°ë§ (1ì£¼ì¼)
- [ ] ë¬¸ì„œ ì—…ë°ì´íŠ¸

---

## ğŸš€ ê²°ë¡ 

í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡°ëŠ” **ê¸°ëŠ¥ì ìœ¼ë¡œ ì™„ë²½**í•˜ì§€ë§Œ, **ì„±ëŠ¥ ìµœì í™”ê°€ ì‹œê¸‰**í•©ë‹ˆë‹¤.

### í•µì‹¬ ê°œì„  ì‚¬í•­
1. **N+1 ì¿¼ë¦¬ í•´ê²°**: ê°€ì¥ í° ì„±ëŠ¥ ë³‘ëª© (85% ê°œì„ )
2. **ë³µí•© ì¸ë±ìŠ¤**: ì¿¼ë¦¬ ì†ë„ 3-5ë°° í–¥ìƒ
3. **JSONB ì „í™˜**: JSON ê²€ìƒ‰ 95% ê°œì„ 
4. **ë©”ëª¨ë¦¬ ì •ë¦¬**: ìŠ¤í† ë¦¬ì§€ 86% ì ˆê°
5. **ì§‘ê³„ í…Œì´ë¸”**: ëŒ€ì‹œë³´ë“œ 98% ê°œì„ 

### ì˜ˆìƒ ì´ íš¨ê³¼
- **ì‘ë‹µ ì‹œê°„**: í‰ê·  86% ê°œì„ 
- **DB ë¶€í•˜**: 81% ê°ì†Œ
- **ì²˜ë¦¬ ìš©ëŸ‰**: 5ë°° ì¦ê°€
- **ë¹„ìš© ì ˆê°**: 70% ê°ì†Œ

### ë‹¤ìŒ ë‹¨ê³„
1. **ì¦‰ì‹œ ì‹œì‘**: N+1 ì¿¼ë¦¬ í•´ê²° (30ë¶„ ì‘ì—…)
2. **Week 1 ì™„ë£Œ**: Critical issues ëª¨ë‘ í•´ê²°
3. **Week 2 ì™„ë£Œ**: Important issues í•´ê²°
4. **ê²€ì¦**: ì„±ëŠ¥ ì¸¡ì • ë° ëª¨ë‹ˆí„°ë§

**ì´ ê°œì„  ì‘ì—…ì„ í†µí•´ ì‹œìŠ¤í…œì´ í˜„ì¬ 100ëª… â†’ 500ëª… ë™ì‹œ ì‚¬ìš©ìë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.**
