"""
ColPali Image Processor for Multimodal RAG

ColPali (Contextualized Late Interaction over Patches) Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨:
- ÏµúÍ≥† ÏàòÏ§ÄÏùò Ï†ïÌôïÎèÑ (nDCG@5: 81.3)
- Îπ†Î•∏ Ïù∏Îç±Ïã± (ÌéòÏù¥ÏßÄÎãπ ~2.5Ï¥à)
- Ìëú/Ï∞®Ìä∏ ÏôÑÎ≤Ω Ïù∏Ïãù
- Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî (Î∞îÏù¥ÎÑàÎ¶¨Ìôî Ïãú 32Î∞∞ Í∞êÏÜå)
- Í≤ÄÏÉâ ÏÜçÎèÑ ÏµúÏ†ÅÌôî (ÌíÄÎßÅ+Î¶¨Îû≠ÌÇπÏúºÎ°ú 13Î∞∞ Ìñ•ÏÉÅ)

Ïö∞ÏÑ†ÏàúÏúÑ: ColPali > Vision LLM > EasyOCR
"""

import logging
import torch
import numpy as np
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
from PIL import Image
import io

logger = logging.getLogger(__name__)


class ColPaliProcessor:
    """
    ColPali Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨Í∏∞
    
    Features:
    - Ìå®Ïπò Í∏∞Î∞ò Î©ÄÌã∞Î≤°ÌÑ∞ ÏûÑÎ≤†Îî©
    - Late interaction Í≤ÄÏÉâ
    - Î∞îÏù¥ÎÑàÎ¶¨Ìôî ÏµúÏ†ÅÌôî
    - ÌíÄÎßÅ + Î¶¨Îû≠ÌÇπ
    - GPU Í∞ÄÏÜç
    """
    
    def __init__(
        self,
        model_name: str = "vidore/colpali-v1.2",
        use_gpu: bool = True,
        enable_binarization: bool = True,
        enable_pooling: bool = True,
        pooling_factor: int = 9  # 1024 patches ‚Üí ~128 patches
    ):
        """
        Ï¥àÍ∏∞Ìôî
        
        Args:
            model_name: ColPali Î™®Îç∏ Ïù¥Î¶Ñ
            use_gpu: GPU ÏÇ¨Ïö© Ïó¨Î∂Ä
            enable_binarization: Î∞îÏù¥ÎÑàÎ¶¨Ìôî ÌôúÏÑ±Ìôî (Î©îÎ™®Î¶¨ 32Î∞∞ Í∞êÏÜå)
            enable_pooling: ÌíÄÎßÅ ÌôúÏÑ±Ìôî (ÏÜçÎèÑ 13Î∞∞ Ìñ•ÏÉÅ)
            pooling_factor: ÌíÄÎßÅ ÎπÑÏú® (9 = 3x3 Ìå®Ïπò ÌíÄÎßÅ)
        """
        self.model_name = model_name
        self.use_gpu = use_gpu and torch.cuda.is_available()
        self.enable_binarization = enable_binarization
        self.enable_pooling = enable_pooling
        self.pooling_factor = pooling_factor
        
        self.model = None
        self.processor = None
        self.device = None
        
        # Lazy loading
        self._init_model()
        
        logger.info(
            f"ColPaliProcessor initialized: "
            f"model={model_name}, gpu={self.use_gpu}, "
            f"binarization={enable_binarization}, pooling={enable_pooling}"
        )
    
    def _init_model(self):
        """Î™®Îç∏ Ï¥àÍ∏∞Ìôî (lazy loading)"""
        try:
            logger.info("üîÑ Importing ColPali modules...")
            from colpali_engine.models import ColPali, ColPaliProcessor
            logger.info("‚úÖ ColPali modules imported successfully")
            
            # GPU Í∞ÄÏö©ÏÑ± Ï≤¥ÌÅ¨
            gpu_available = torch.cuda.is_available()
            if gpu_available:
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                logger.info(f"üéÆ GPU detected: {gpu_name} ({gpu_memory:.1f}GB)")
            else:
                logger.info("üíª No GPU detected, using CPU")
            
            # ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï
            self.device = torch.device("cuda" if self.use_gpu and gpu_available else "cpu")
            
            if self.use_gpu and not gpu_available:
                logger.warning("‚ö†Ô∏è  GPU requested but not available, falling back to CPU")
            
            logger.info(f"üì¶ Loading ColPali model: {self.model_name}")
            logger.info(f"üîß Device: {self.device}")
            
            # Î™®Îç∏ Î°úÎìú
            self.model = ColPali.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 if self.use_gpu and gpu_available else torch.float32,
                device_map="auto" if self.use_gpu and gpu_available else None
            ).eval()
            
            # GPUÎ°ú Î™ÖÏãúÏ†Å Ïù¥Îèô (device_mapÏù¥ ÏûëÎèôÌïòÏßÄ ÏïäÏùÑ Í≤ΩÏö∞ ÎåÄÎπÑ)
            if self.use_gpu and gpu_available:
                self.model = self.model.to(self.device)
            
            # ÌîÑÎ°úÏÑ∏ÏÑú Î°úÎìú
            self.processor = ColPaliProcessor.from_pretrained(self.model_name)
            
            # ÏµúÏ¢Ö ÌôïÏù∏
            actual_device = next(self.model.parameters()).device
            logger.info(f"‚úÖ ColPali model loaded successfully")
            logger.info(f"   Device: {actual_device}")
            logger.info(f"   Dtype: {next(self.model.parameters()).dtype}")
            logger.info(f"   Binarization: {self.enable_binarization}")
            logger.info(f"   Pooling: {self.enable_pooling} (factor={self.pooling_factor})")
            
        except ImportError as e:
            logger.error(
                f"‚ùå ColPali import failed: {e}. "
                "Install with: pip install colpali-engine"
            )
            raise ImportError(
                f"ColPali not installed. Please install: pip install colpali-engine. Error: {e}"
            )
        except Exception as e:
            logger.error(f"Failed to initialize ColPali model: {e}")
            raise
    
    def process_image(
        self,
        image_path: str,
        return_embeddings: bool = True
    ) -> Dict[str, Any]:
        """
        Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨ Î∞è ÏûÑÎ≤†Îî© ÏÉùÏÑ±
        
        Args:
            image_path: Ïù¥ÎØ∏ÏßÄ ÌååÏùº Í≤ΩÎ°ú
            return_embeddings: ÏûÑÎ≤†Îî© Î∞òÌôò Ïó¨Î∂Ä
        
        Returns:
            {
                'embeddings': np.ndarray,  # Ìå®Ïπò ÏûÑÎ≤†Îî© (N, D)
                'num_patches': int,        # Ìå®Ïπò Ïàò
                'image_size': Tuple,       # Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞
                'method': str,             # 'colpali'
                'confidence': float,       # Ïã†Î¢∞ÎèÑ
                'metadata': Dict           # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞
            }
        """
        if not Path(image_path).exists():
            raise FileNotFoundError(f"Image not found: {image_path}")
        
        try:
            # Check if it's a PDF file
            if image_path.lower().endswith('.pdf'):
                # Convert PDF to images
                import fitz  # PyMuPDF
                doc = fitz.open(image_path)
                
                # Process first page only for now
                page = doc[0]
                pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2x scale for better quality
                
                # Convert to PIL Image
                import io
                img_data = pix.tobytes("png")
                image = Image.open(io.BytesIO(img_data)).convert("RGB")
                image_size = image.size
                
                doc.close()
            else:
                # Ïù¥ÎØ∏ÏßÄ Î°úÎìú
                image = Image.open(image_path).convert("RGB")
                image_size = image.size
            
            # Ï†ÑÏ≤òÎ¶¨
            inputs = self.processor(
                images=image,
                return_tensors="pt"
            )
            
            # GPUÎ°ú Ïù¥Îèô (Î™ÖÏãúÏ†Å)
            if self.use_gpu and torch.cuda.is_available():
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ÏûÑÎ≤†Îî© ÏÉùÏÑ±
            import time
            start_time = time.time()
            
            with torch.no_grad():
                embeddings = self.model(**inputs)
            
            processing_time = time.time() - start_time
            
            # CPUÎ°ú Ïù¥Îèô Î∞è numpy Î≥ÄÌôò
            embeddings = embeddings.cpu().numpy()
            
            logger.debug(f"‚ö° Processing time: {processing_time:.2f}s on {self.device}")
            
            # ÏµúÏ†ÅÌôî Ï†ÅÏö©
            if self.enable_pooling:
                embeddings = self._apply_pooling(embeddings)
            
            if self.enable_binarization:
                embeddings = self._apply_binarization(embeddings)
            
            num_patches = embeddings.shape[0]
            
            logger.info(
                f"‚úÖ ColPali processed image: {num_patches} patches, "
                f"shape={embeddings.shape}, device={self.device}"
            )
            
            result = {
                'embeddings': embeddings if return_embeddings else None,
                'num_patches': num_patches,
                'image_size': image_size,
                'method': 'colpali',
                'confidence': 0.95,  # ColPaliÎäî ÎÜíÏùÄ Ïã†Î¢∞ÎèÑ
                'metadata': {
                    'model': self.model_name,
                    'device': str(self.device),
                    'binarized': self.enable_binarization,
                    'pooled': self.enable_pooling,
                    'pooling_factor': self.pooling_factor if self.enable_pooling else None
                }
            }
            
            return result
            
        except Exception as e:
            logger.error(f"ColPali processing failed for {image_path}: {e}")
            raise
    
    def _apply_pooling(self, embeddings: np.ndarray) -> np.ndarray:
        """
        Ìå®Ïπò ÌíÄÎßÅ Ï†ÅÏö©
        
        1024 patches ‚Üí ~128 patches (9x reduction)
        ÏÜçÎèÑ 13Î∞∞ Ìñ•ÏÉÅ, Ï†ïÌôïÎèÑ Ïú†ÏßÄ
        
        Args:
            embeddings: ÏõêÎ≥∏ Ìå®Ïπò ÏûÑÎ≤†Îî© (N, D)
        
        Returns:
            ÌíÄÎßÅÎêú ÏûÑÎ≤†Îî© (N/pooling_factor, D)
        """
        try:
            N, D = embeddings.shape
            
            # ÌíÄÎßÅ Ìå©ÌÑ∞Î°ú ÎÇòÎàÑÏñ¥Îñ®Ïñ¥ÏßÄÎèÑÎ°ù Ï°∞Ï†ï
            num_pooled = N // self.pooling_factor
            
            if num_pooled == 0:
                logger.warning("Too few patches for pooling, skipping")
                return embeddings
            
            # Í∑∏Î£πÏúºÎ°ú ÎÇòÎàÑÍ∏∞
            embeddings_grouped = embeddings[:num_pooled * self.pooling_factor].reshape(
                num_pooled, self.pooling_factor, D
            )
            
            # ÌèâÍ∑† ÌíÄÎßÅ
            pooled = embeddings_grouped.mean(axis=1)
            
            logger.debug(f"Pooling applied: {N} ‚Üí {num_pooled} patches")
            
            return pooled
            
        except Exception as e:
            logger.warning(f"Pooling failed: {e}, using original embeddings")
            return embeddings
    
    def _apply_binarization(self, embeddings: np.ndarray) -> np.ndarray:
        """
        ÏûÑÎ≤†Îî© Î∞îÏù¥ÎÑàÎ¶¨Ìôî
        
        Î©îÎ™®Î¶¨ 32Î∞∞ Í∞êÏÜå, ÏÜçÎèÑ 4Î∞∞ Ìñ•ÏÉÅ
        
        Args:
            embeddings: ÏõêÎ≥∏ ÏûÑÎ≤†Îî© (N, D)
        
        Returns:
            Î∞îÏù¥ÎÑàÎ¶¨ ÏûÑÎ≤†Îî© (N, D/8) - uint8 packed
        """
        try:
            # 0ÏùÑ Í∏∞Ï§ÄÏúºÎ°ú Î∞îÏù¥ÎÑàÎ¶¨Ìôî
            binary = (embeddings > 0).astype(np.uint8)
            
            # ÎπÑÌä∏ Ìå®ÌÇπ (8Í∞ú Í∞íÏùÑ 1Î∞îÏù¥Ìä∏Î°ú)
            N, D = binary.shape
            D_packed = (D + 7) // 8  # Ïò¨Î¶º
            
            packed = np.zeros((N, D_packed), dtype=np.uint8)
            
            for i in range(D_packed):
                start_idx = i * 8
                end_idx = min(start_idx + 8, D)
                
                for j in range(start_idx, end_idx):
                    bit_pos = j - start_idx
                    packed[:, i] |= (binary[:, j] << bit_pos)
            
            logger.debug(f"Binarization applied: {embeddings.nbytes} ‚Üí {packed.nbytes} bytes")
            
            return packed
            
        except Exception as e:
            logger.warning(f"Binarization failed: {e}, using original embeddings")
            return embeddings
    
    def process_text_query(self, query: str) -> np.ndarray:
        """
        ÌÖçÏä§Ìä∏ ÏøºÎ¶¨Î•º ColPali ÏûÑÎ≤†Îî©ÏúºÎ°ú Î≥ÄÌôò
        
        Args:
            query: Í≤ÄÏÉâ ÏøºÎ¶¨ ÌÖçÏä§Ìä∏
        
        Returns:
            ÏøºÎ¶¨ ÏûÑÎ≤†Îî© (M, D)
        """
        try:
            # PaliGemmaProcessorÎäî Ïù¥ÎØ∏ÏßÄÍ∞Ä ÌïÑÏöîÌïòÎØÄÎ°ú ÎçîÎØ∏ Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ±
            # ÎòêÎäî ÌÖçÏä§Ìä∏ Ï†ÑÏö© Ï≤òÎ¶¨ Î∞©Ïãù ÏÇ¨Ïö©
            from PIL import Image
            import numpy as np
            
            # ÏûëÏùÄ ÎçîÎØ∏ Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ± (1x1 Ìù∞ÏÉâ Ïù¥ÎØ∏ÏßÄ)
            dummy_image = Image.new('RGB', (1, 1), color='white')
            
            # ÌÖçÏä§Ìä∏ÏôÄ ÎçîÎØ∏ Ïù¥ÎØ∏ÏßÄÎ°ú Ï†ÑÏ≤òÎ¶¨
            inputs = self.processor(
                text=query,
                images=dummy_image,
                return_tensors="pt"
            ).to(self.device)
            
            # ÏûÑÎ≤†Îî© ÏÉùÏÑ±
            with torch.no_grad():
                embeddings = self.model(**inputs)
            
            # CPUÎ°ú Ïù¥Îèô Î∞è numpy Î≥ÄÌôò
            embeddings = embeddings.cpu().numpy()
            
            # ÏµúÏ†ÅÌôî Ï†ÅÏö©
            if self.enable_pooling:
                embeddings = self._apply_pooling(embeddings)
            
            if self.enable_binarization:
                embeddings = self._apply_binarization(embeddings)
            
            logger.debug(f"Query embedding generated: shape={embeddings.shape}")
            
            return embeddings
            
        except Exception as e:
            logger.error(f"Query processing failed: {e}")
            raise
    
    def compute_similarity(
        self,
        query_embedding: np.ndarray,
        doc_embeddings: np.ndarray,
        use_late_interaction: bool = True
    ) -> float:
        """
        Late interaction Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞
        
        MaxSim: max_i(query_i ¬∑ doc_j) for each query patch
        
        Args:
            query_embedding: ÏøºÎ¶¨ ÏûÑÎ≤†Îî© (M, D)
            doc_embeddings: Î¨∏ÏÑú ÏûÑÎ≤†Îî© (N, D)
            use_late_interaction: Late interaction ÏÇ¨Ïö© Ïó¨Î∂Ä
        
        Returns:
            Ïú†ÏÇ¨ÎèÑ Ï†êÏàò
        """
        try:
            if use_late_interaction:
                # Late interaction (MaxSim)
                # Í∞Å ÏøºÎ¶¨ Ìå®ÏπòÏóê ÎåÄÌï¥ Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Î¨∏ÏÑú Ìå®Ïπò Ï∞æÍ∏∞
                similarities = np.dot(query_embedding, doc_embeddings.T)  # (M, N)
                max_sims = similarities.max(axis=1)  # (M,)
                score = max_sims.sum()  # ÎòêÎäî mean()
            else:
                # Îã®Ïàú ÌèâÍ∑† Ïú†ÏÇ¨ÎèÑ
                query_avg = query_embedding.mean(axis=0)
                doc_avg = doc_embeddings.mean(axis=0)
                score = np.dot(query_avg, doc_avg)
            
            return float(score)
            
        except Exception as e:
            logger.error(f"Similarity computation failed: {e}")
            return 0.0
    
    def batch_process_images(
        self,
        image_paths: List[str],
        batch_size: int = 4
    ) -> List[Dict[str, Any]]:
        """
        Î∞∞Ïπò Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨
        
        Args:
            image_paths: Ïù¥ÎØ∏ÏßÄ Í≤ΩÎ°ú Î¶¨Ïä§Ìä∏
            batch_size: Î∞∞Ïπò ÌÅ¨Í∏∞
        
        Returns:
            Ï≤òÎ¶¨ Í≤∞Í≥º Î¶¨Ïä§Ìä∏
        """
        results = []
        
        for i in range(0, len(image_paths), batch_size):
            batch_paths = image_paths[i:i + batch_size]
            
            for path in batch_paths:
                try:
                    result = self.process_image(path)
                    results.append(result)
                except Exception as e:
                    logger.error(f"Failed to process {path}: {e}")
                    results.append({
                        'error': str(e),
                        'method': 'colpali',
                        'confidence': 0.0
                    })
        
        return results
    
    def is_available(self) -> bool:
        """ColPali ÏÇ¨Ïö© Í∞ÄÎä• Ïó¨Î∂Ä ÌôïÏù∏"""
        try:
            return self.model is not None and self.processor is not None
        except:
            return False
    
    def get_model_info(self) -> Dict[str, Any]:
        """Î™®Îç∏ Ï†ïÎ≥¥ Î∞òÌôò"""
        return {
            'model_name': self.model_name,
            'device': str(self.device) if self.device else 'cpu',
            'gpu_available': torch.cuda.is_available(),
            'gpu_used': self.use_gpu,
            'binarization': self.enable_binarization,
            'pooling': self.enable_pooling,
            'pooling_factor': self.pooling_factor,
            'available': self.is_available()
        }


# Global instance
_colpali_processor: Optional[ColPaliProcessor] = None


def get_colpali_processor(
    model_name: str = "vidore/colpali-v1.2",
    use_gpu: bool = True,
    enable_binarization: bool = True,
    enable_pooling: bool = True,
    pooling_factor: int = 9
) -> Optional[ColPaliProcessor]:
    """
    Get global ColPali processor instance
    
    Returns None if ColPali is not available
    """
    global _colpali_processor
    
    if _colpali_processor is None:
        try:
            _colpali_processor = ColPaliProcessor(
                model_name=model_name,
                use_gpu=use_gpu,
                enable_binarization=enable_binarization,
                enable_pooling=enable_pooling,
                pooling_factor=pooling_factor
            )
        except Exception as e:
            logger.warning(f"ColPali not available: {e}")
            return None
    
    return _colpali_processor
