"""
Web Search + RAG Hybrid Agent

RAG ê²€ìƒ‰ê³¼ ì›¹ ê²€ìƒ‰ì„ ê²°í•©í•˜ì—¬ ì¢…í•©ì ì¸ ë¶„ì„ì„ ì œê³µí•˜ëŠ” ì—ì´ì „íŠ¸
"""

import logging
import asyncio
import hashlib
from typing import List, Dict, Any, AsyncGenerator, Optional, Tuple
from datetime import datetime, timedelta
import json

from backend.services.web_search_service import get_web_search_service
from backend.services.embedding import EmbeddingService
from backend.services.milvus import MilvusManager
from backend.services.llm_manager import LLMManager
from backend.services.web_search_enhancer import (
    get_credibility_scorer,
    get_deduplicator,
    get_temporal_filter
)
from backend.services.query_enhancer import get_query_enhancer
from backend.services.adaptive_reranker import get_adaptive_reranker
from backend.models.query import SearchResult
from backend.models.chunk_types import ChunkType, StepType, SourceType
from backend.config import settings

logger = logging.getLogger(__name__)


class WebSearchAgent:
    """
    Web Search + RAG í•˜ì´ë¸Œë¦¬ë“œ ì—ì´ì „íŠ¸
    
    Features:
    - RAG ê²€ìƒ‰ (ë‚´ë¶€ ë¬¸ì„œ)
    - Web ê²€ìƒ‰ (ì™¸ë¶€ ì •ë³´)
    - ê²°ê³¼ í†µí•© ë° ë¶„ì„
    - ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
    """
    
    def __init__(
        self,
        embedding_service: EmbeddingService,
        milvus_manager: MilvusManager,
        llm_manager: LLMManager
    ) -> None:
        self.embedding_service: EmbeddingService = embedding_service
        self.milvus_manager: MilvusManager = milvus_manager
        self.llm_manager: LLMManager = llm_manager
        self.web_search_service = get_web_search_service()
        
        # ì›¹ ê²€ìƒ‰ ìºì‹œ (ë©”ëª¨ë¦¬ ê¸°ë°˜)
        self.search_cache: Dict[str, Tuple[List[Dict[str, Any]], datetime]] = {}
        self.cache_ttl: timedelta = timedelta(hours=1)  # 1ì‹œê°„ TTL
        self.max_cache_size: int = 100  # ìµœëŒ€ 100ê°œ ìºì‹œ
        
        # ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ ì»´í¬ë„ŒíŠ¸ (Phase 1)
        self.credibility_scorer = get_credibility_scorer()
        self.deduplicator = get_deduplicator()
        self.temporal_filter = get_temporal_filter()
        
        # Phase 2 ì»´í¬ë„ŒíŠ¸
        self.query_enhancer = get_query_enhancer(llm_manager)
        self.reranker = get_adaptive_reranker()
        
        logger.info(
            "WebSearchAgent initialized with Phase 1 & 2 enhancements: "
            "credibility, dedup, temporal, query enhancement, reranking"
        )
    
    async def process_query(
        self,
        query: str,
        session_id: str,
        top_k: int = 10,
        web_results: int = 5
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ì¿¼ë¦¬ ì²˜ë¦¬ (RAG + Web Search)
        
        Args:
            query: ì‚¬ìš©ì ì¿¼ë¦¬
            session_id: ì„¸ì…˜ ID
            top_k: RAG ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
            web_results: ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
            
        Yields:
            ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²­í¬
        """
        start_time = datetime.now()
        
        try:
            # 1. ì§„í–‰ ìƒí™© ì•Œë¦¼
            yield {
                "type": ChunkType.STEP.value,
                "data": {
                    "step_id": f"web_search_start_{session_id}",
                    "type": StepType.PLANNING.value,
                    "content": "ğŸ” RAG ê²€ìƒ‰ê³¼ ì›¹ ê²€ìƒ‰ì„ ë™ì‹œì— ì‹œì‘í•©ë‹ˆë‹¤...",
                    "timestamp": datetime.now().isoformat(),
                    "metadata": {"step": "start"}
                }
            }
            
            # 2. RAG ê²€ìƒ‰ê³¼ Web ê²€ìƒ‰ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰ (íƒ€ì„ì•„ì›ƒ ì¶”ê°€)
            rag_task = asyncio.wait_for(self._search_rag(query, top_k), timeout=5.0)
            web_task = asyncio.wait_for(self._search_web(query, web_results), timeout=10.0)
            
            rag_results, web_results_data = await asyncio.gather(
                rag_task,
                web_task,
                return_exceptions=True
            )
            
            # ì—ëŸ¬ ì²˜ë¦¬ ë° ì‚¬ìš©ì ì•Œë¦¼
            if isinstance(rag_results, Exception):
                logger.error(f"RAG search failed: {rag_results}")
                rag_results = []
                yield {
                    "type": ChunkType.STEP.value,
                    "data": {
                        "step_id": f"rag_search_failed_{session_id}",
                        "type": StepType.WARNING.value,
                        "content": "âš ï¸ ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ ê²°ê³¼ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.",
                        "timestamp": datetime.now().isoformat(),
                        "metadata": {"step": "rag_failed"}
                    }
                }
            
            if isinstance(web_results_data, Exception):
                logger.error(f"Web search failed: {web_results_data}")
                web_results_data = []
                yield {
                    "type": ChunkType.STEP.value,
                    "data": {
                        "step_id": f"web_search_failed_{session_id}",
                        "type": StepType.WARNING.value,
                        "content": "âš ï¸ ì›¹ ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‚´ë¶€ ë¬¸ì„œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.",
                        "timestamp": datetime.now().isoformat(),
                        "metadata": {"step": "web_failed"}
                    }
                }
            
            # ë‘˜ ë‹¤ ì‹¤íŒ¨í•œ ê²½ìš°
            if not rag_results and not web_results_data:
                yield {
                    "type": ChunkType.ERROR.value,
                    "data": {
                        "error": "RAG ê²€ìƒ‰ê³¼ ì›¹ ê²€ìƒ‰ ëª¨ë‘ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                        "error_type": "all_failed",
                        "timestamp": datetime.now().isoformat()
                    }
                }
                return
            
            # 3. ê²€ìƒ‰ ê²°ê³¼ ì•Œë¦¼
            yield {
                "type": ChunkType.STEP.value,
                "data": {
                    "step_id": f"web_search_results_{session_id}",
                    "type": StepType.ACTION.value,
                    "content": f"âœ… RAG: {len(rag_results)}ê°œ, Web: {len(web_results_data)}ê°œ ê²°ê³¼ ë°œê²¬",
                    "timestamp": datetime.now().isoformat(),
                    "metadata": {
                        "step": "search_complete",
                        "rag_count": len(rag_results),
                        "web_count": len(web_results_data)
                    }
                }
            }
            
            # 4. ê²°ê³¼ í†µí•© ë° ë¶„ì„
            yield {
                "type": ChunkType.STEP.value,
                "data": {
                    "step_id": f"web_search_analysis_{session_id}",
                    "type": StepType.THOUGHT.value,
                    "content": "ğŸ¤– AIê°€ RAGì™€ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¢…í•© ë¶„ì„ ì¤‘...",
                    "timestamp": datetime.now().isoformat(),
                    "metadata": {"step": "analysis"}
                }
            }
            
            # 5. LLMìœ¼ë¡œ í†µí•© ë¶„ì„
            async for chunk in self._generate_hybrid_response(
                query=query,
                rag_results=rag_results,
                web_results=web_results_data
            ):
                yield chunk
            
            # 6. ì™„ë£Œ
            total_time: float = (datetime.now() - start_time).total_seconds()
            yield {
                "type": ChunkType.DONE.value,
                "data": {
                    "message": "âœ… ë¶„ì„ ì™„ë£Œ",
                    "total_time": total_time,
                    "timestamp": datetime.now().isoformat()
                }
            }
            
        except asyncio.TimeoutError as e:
            logger.error(f"WebSearchAgent timeout: {e}", exc_info=True)
            yield {
                "type": ChunkType.ERROR.value,
                "data": {
                    "error": "ì²˜ë¦¬ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                    "error_type": "timeout",
                    "timestamp": datetime.now().isoformat()
                }
            }
        except (ConnectionError, OSError) as e:
            logger.error(f"WebSearchAgent connection error: {e}", exc_info=True)
            yield {
                "type": ChunkType.ERROR.value,
                "data": {
                    "error": "ì„œë¹„ìŠ¤ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                    "error_type": "connection",
                    "timestamp": datetime.now().isoformat()
                }
            }
        except Exception as e:
            logger.error(f"WebSearchAgent unexpected error: {e}", exc_info=True)
            yield {
                "type": ChunkType.ERROR.value,
                "data": {
                    "error": f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                    "error_type": "unexpected",
                    "timestamp": datetime.now().isoformat()
                }
            }
    
    async def _search_rag(self, query: str, top_k: int) -> List[SearchResult]:
        """RAG ê²€ìƒ‰ ì‹¤í–‰"""
        try:
            logger.info(f"RAG search: '{query[:50]}...' (top_k={top_k})")
            
            # ì¿¼ë¦¬ ì„ë² ë”©
            query_embedding: List[float] = await self.embedding_service.embed_query(query)
            
            # Milvus ê²€ìƒ‰
            results: List[Dict[str, Any]] = await self.milvus_manager.search(
                query_embedding=query_embedding,
                top_k=top_k
            )
            
            # SearchResult í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            search_results: List[SearchResult] = []
            for result in results:
                search_results.append(SearchResult(
                    document_id=result.get("document_id", ""),
                    chunk_id=result.get("chunk_id", ""),
                    content=result.get("content", ""),
                    score=result.get("score", 0.0),
                    metadata=result.get("metadata", {})
                ))
            
            logger.info(f"RAG search completed: {len(search_results)} results")
            return search_results
            
        except (ConnectionError, TimeoutError) as e:
            logger.error(f"RAG search connection/timeout error: {e}", exc_info=True)
            return []
        except ValueError as e:
            logger.error(f"Invalid RAG search parameters: {e}")
            return []
        except Exception as e:
            logger.error(f"Unexpected RAG search error: {e}", exc_info=True)
            return []
    
    async def _search_web(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """ì›¹ ê²€ìƒ‰ ì‹¤í–‰ (ìºì‹± + í’ˆì§ˆ í–¥ìƒ í¬í•¨)"""
        try:
            # ìºì‹œ í‚¤ ìƒì„±
            cache_key: str = hashlib.md5(f"{query}:{max_results}".encode()).hexdigest()
            
            # ìºì‹œ í™•ì¸
            if cache_key in self.search_cache:
                cached_results, timestamp = self.search_cache[cache_key]
                if datetime.now() - timestamp < self.cache_ttl:
                    logger.debug(f"Web search cache hit: '{query[:50]}...'")
                    return cached_results
                else:
                    # ë§Œë£Œëœ ìºì‹œ ì œê±°
                    del self.search_cache[cache_key]
            
            # ìºì‹œ ë¯¸ìŠ¤ - ì‹¤ì œ ê²€ìƒ‰ ìˆ˜í–‰
            logger.info(f"Web search: '{query[:50]}...' (max_results={max_results})")
            
            # Phase 2: ë‹¤ì¤‘ ì¿¼ë¦¬ ì „ëµ ì‚¬ìš©
            raw_results: List[Dict[str, Any]] = await self._multi_query_search(
                query=query,
                max_results_per_query=max_results,  # ì¿¼ë¦¬ë‹¹ ê²°ê³¼ ìˆ˜
                enable_query_enhancement=True
            )
            
            # í’ˆì§ˆ í–¥ìƒ íŒŒì´í”„ë¼ì¸ ì ìš© (Phase 1 + Phase 2)
            enhanced_results = await self._enhance_search_results(
                raw_results,
                max_results,
                query=query,  # ì¬ìˆœìœ„í™”ë¥¼ ìœ„í•´ ì¿¼ë¦¬ ì „ë‹¬
                enable_reranking=True
            )
            
            # ìºì‹œ ì €ì¥ (í¬ê¸° ì œí•œ)
            if len(self.search_cache) >= self.max_cache_size:
                # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±° (LRU)
                oldest_key: str = min(
                    self.search_cache.keys(), 
                    key=lambda k: self.search_cache[k][1]
                )
                del self.search_cache[oldest_key]
            
            self.search_cache[cache_key] = (enhanced_results, datetime.now())
            
            logger.info(
                f"Web search completed: {len(raw_results)} -> {len(enhanced_results)} "
                f"results (enhanced & cached)"
            )
            return enhanced_results
            
        except (ConnectionError, TimeoutError) as e:
            logger.error(f"Web search connection/timeout error: {e}", exc_info=True)
            return []
        except ValueError as e:
            logger.error(f"Invalid web search parameters: {e}")
            return []
        except Exception as e:
            logger.error(f"Unexpected web search error: {e}", exc_info=True)
            return []
    
    async def _enhance_search_results(
        self,
        results: List[Dict[str, Any]],
        max_results: int,
        query: Optional[str] = None,
        enable_reranking: bool = True
    ) -> List[Dict[str, Any]]:
        """
        ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í–¥ìƒ íŒŒì´í”„ë¼ì¸ (Phase 1 + Phase 2)
        
        Phase 1:
        1. ì†ŒìŠ¤ ì‹ ë¢°ë„ í•„í„°ë§
        2. ì¤‘ë³µ ì œê±°
        3. ì‹œê°„ ê¸°ë°˜ í•„í„°ë§
        
        Phase 2:
        4. ì¬ìˆœìœ„í™” (Cross-encoder)
        5. ìƒìœ„ Nê°œ ì„ íƒ
        """
        if not results:
            return []
        
        try:
            # Phase 1: ê¸°ë³¸ í•„í„°ë§
            # 1. ì†ŒìŠ¤ ì‹ ë¢°ë„ í•„í„°ë§ (ìµœì†Œ 0.5)
            credible_results = self.credibility_scorer.filter_by_credibility(
                results,
                min_score=0.5
            )
            
            # 2. ì¤‘ë³µ ì œê±° (URL + ì½˜í…ì¸ )
            unique_results = self.deduplicator.deduplicate(
                credible_results,
                method="both"
            )
            
            # 3. ì‹œê°„ ê¸°ë°˜ í•„í„°ë§ (ìµœì‹  ìš°ì„ )
            filtered_results = self.temporal_filter.filter_by_recency(
                unique_results,
                prefer_recent=True,
                max_age_days=None,  # ë‚˜ì´ ì œí•œ ì—†ìŒ
                boost_recent=True  # ìµœì‹  ê²°ê³¼ì— ì ìˆ˜ ë¶€ìŠ¤íŠ¸
            )
            
            # Phase 2: ì¬ìˆœìœ„í™”
            if enable_reranking and query and filtered_results:
                try:
                    # ì¬ìˆœìœ„í™”ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
                    for result in filtered_results:
                        # 'text' í•„ë“œê°€ ì—†ìœ¼ë©´ 'snippet' + 'title' ì‚¬ìš©
                        if 'text' not in result:
                            result['text'] = f"{result.get('title', '')} {result.get('snippet', '')}"
                    
                    # ì¬ìˆœìœ„í™” (ë” ë§ì€ ê²°ê³¼ë¥¼ ì¬ìˆœìœ„í™” í›„ ìƒìœ„ ì„ íƒ)
                    reranked_results = await self.reranker.rerank(
                        query=query,
                        results=filtered_results,
                        top_k=max_results * 2,  # 2ë°° ì¬ìˆœìœ„í™” í›„ ì„ íƒ
                        score_threshold=0.0
                    )
                    
                    logger.info(
                        f"Reranked {len(filtered_results)} -> {len(reranked_results)} results"
                    )
                    
                    filtered_results = reranked_results
                    
                except Exception as e:
                    logger.warning(f"Reranking failed, using Phase 1 results: {e}")
            
            # 4. ì¢…í•© ì ìˆ˜ë¡œ ì •ë ¬ (ì‹ ë¢°ë„ + ìµœì‹ ì„± + ì¬ìˆœìœ„ ì ìˆ˜)
            for result in filtered_results:
                credibility = result.get('credibility_score', 0.5)
                recency = result.get('recency_score', 0.5)
                rerank_score = result.get('score', 0.5)  # ì¬ìˆœìœ„ ì ìˆ˜
                
                # ì¬ìˆœìœ„í™”ê°€ ì ìš©ë˜ì—ˆìœ¼ë©´ ì¬ìˆœìœ„ ì ìˆ˜ ìš°ì„ 
                if result.get('reranked'):
                    # ê°€ì¤‘ í‰ê·  (ì¬ìˆœìœ„ 60%, ì‹ ë¢°ë„ 30%, ìµœì‹ ì„± 10%)
                    result['quality_score'] = (
                        rerank_score * 0.6 +
                        credibility * 0.3 +
                        recency * 0.1
                    )
                else:
                    # ì¬ìˆœìœ„í™” ì—†ìœ¼ë©´ Phase 1 ì ìˆ˜
                    result['quality_score'] = credibility * 0.7 + recency * 0.3
            
            filtered_results.sort(
                key=lambda x: x.get('quality_score', 0),
                reverse=True
            )
            
            # 5. ìƒìœ„ Nê°œ ì„ íƒ
            final_results = filtered_results[:max_results]
            
            logger.info(
                f"Search enhancement pipeline (Phase 1+2): "
                f"{len(results)} -> {len(credible_results)} (credibility) "
                f"-> {len(unique_results)} (dedup) "
                f"-> {len(filtered_results)} (temporal+rerank) "
                f"-> {len(final_results)} (final)"
            )
            
            return final_results
            
        except Exception as e:
            logger.error(f"Error enhancing search results: {e}", exc_info=True)
            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê²°ê³¼ ë°˜í™˜
            return results[:max_results]
    
    async def _multi_query_search(
        self,
        query: str,
        max_results_per_query: int = 5,
        enable_query_enhancement: bool = True
    ) -> List[Dict[str, Any]]:
        """
        ë‹¤ì¤‘ ì¿¼ë¦¬ ì „ëµìœ¼ë¡œ ê²€ìƒ‰ (Phase 2)
        
        ì—¬ëŸ¬ ë³€í˜• ì¿¼ë¦¬ë¡œ ê²€ìƒ‰í•˜ì—¬ ê²°ê³¼ í†µí•©
        
        Args:
            query: ì›ë³¸ ì¿¼ë¦¬
            max_results_per_query: ì¿¼ë¦¬ë‹¹ ìµœëŒ€ ê²°ê³¼ ìˆ˜
            enable_query_enhancement: ì¿¼ë¦¬ í™•ì¥ í™œì„±í™”
            
        Returns:
            í†µí•©ëœ ê²€ìƒ‰ ê²°ê³¼
        """
        try:
            # ì¿¼ë¦¬ í™•ì¥
            if enable_query_enhancement:
                enhanced = await self.query_enhancer.enhance_query(
                    query,
                    enable_llm=True,
                    max_expansions=2
                )
                
                # ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„±
                query_variants = [
                    query,  # ì›ë³¸
                    enhanced['rewritten'],  # ì¬ì‘ì„±
                ]
                
                # í™•ì¥ ì¿¼ë¦¬ ì¶”ê°€ (ìµœëŒ€ 2ê°œ)
                query_variants.extend(enhanced['expanded'][:2])
                
                # ì˜ë„ ê¸°ë°˜ ì¿¼ë¦¬ ì¶”ê°€
                intent_queries = self.query_enhancer.generate_multi_queries(
                    query,
                    enhanced['intent'],
                    num_queries=3
                )
                query_variants.extend(intent_queries[:2])
                
                # ì¤‘ë³µ ì œê±°
                unique_queries = []
                seen = set()
                for q in query_variants:
                    if q and q not in seen:
                        unique_queries.append(q)
                        seen.add(q)
                
                query_variants = unique_queries[:5]  # ìµœëŒ€ 5ê°œ
                
                logger.info(
                    f"Multi-query search: {len(query_variants)} variants "
                    f"(intent={enhanced['intent']})"
                )
            else:
                query_variants = [query]
            
            # ë³‘ë ¬ ê²€ìƒ‰
            search_tasks = [
                self.web_search_service.search(
                    variant,
                    max_results=max_results_per_query,
                    language="ko",
                    region="kr"
                )
                for variant in query_variants
            ]
            
            all_results_list = await asyncio.gather(*search_tasks, return_exceptions=True)
            
            # ê²°ê³¼ í†µí•©
            all_results = []
            for results in all_results_list:
                if isinstance(results, Exception):
                    logger.warning(f"Query variant failed: {results}")
                    continue
                if isinstance(results, list):
                    all_results.extend(results)
            
            logger.info(
                f"Multi-query search collected {len(all_results)} total results "
                f"from {len(query_variants)} queries"
            )
            
            return all_results
            
        except Exception as e:
            logger.error(f"Multi-query search failed: {e}", exc_info=True)
            # í´ë°±: ë‹¨ì¼ ì¿¼ë¦¬ ê²€ìƒ‰
            return await self.web_search_service.search(
                query,
                max_results=max_results_per_query * 3,
                language="ko",
                region="kr"
            )
    
    async def _generate_hybrid_response(
        self,
        query: str,
        rag_results: List[SearchResult],
        web_results: List[Dict[str, Any]],
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """RAG + Web ê²°ê³¼ë¥¼ í†µí•©í•˜ì—¬ ì‘ë‹µ ìƒì„± (Medium Priority ê°œì„  ì ìš©)"""
        
        # í”„ë¡¬í”„íŠ¸ ìµœì í™” ë° í…œí”Œë¦¿ ê´€ë¦¬
        from backend.core.prompt_optimizer import get_prompt_optimizer
        from backend.core.token_budget_manager import get_token_budget_manager
        from backend.core.response_template import get_response_template_manager, ResponseFormat
        from backend.core.context_optimizer import get_context_optimizer
        
        prompt_optimizer = get_prompt_optimizer()
        budget_manager = get_token_budget_manager(max_tokens=4096)
        template_manager = get_response_template_manager()
        context_optimizer = get_context_optimizer(
            min_relevance_score=0.7,  # ë™ì  ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°
            max_docs=5,
            max_chars_per_doc=400
        )
        
        # 1. ë™ì  ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° - ê´€ë ¨ë„ ê¸°ë°˜ í•„í„°ë§
        if rag_results:
            rag_results = context_optimizer.filter_by_relevance(
                results=rag_results,
                min_score=0.7,
                max_docs=5,
                dynamic_threshold=True  # ë™ì  ì„ê³„ê°’
            )
        
        # 2. ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì»¨í…ìŠ¤íŠ¸ë§Œ)
        user_prompt = self._build_hybrid_prompt(query, rag_results, web_results)
        
        # 3. ì‘ë‹µ í…œí”Œë¦¿ ì ìš©
        template = template_manager.get_template(ResponseFormat.STANDARD)
        user_prompt_with_template = template_manager.build_prompt_with_template(
            query=query,
            context=user_prompt,
            format_type=ResponseFormat.STANDARD
        )
        
        # 4. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì¬ì‚¬ìš© ê°€ëŠ¥, ìºì‹± ê°€ëŠ¥)
        system_prompt = prompt_optimizer.SYSTEM_PROMPTS["web_hybrid"]
        
        # 5. ì˜ˆì‚° ì²´í¬
        fits, total_tokens, message = budget_manager.check_budget(
            system_prompt=system_prompt,
            user_prompt=user_prompt_with_template,
            max_output_tokens=template.max_tokens
        )
        
        if not fits:
            logger.warning(f"Token budget exceeded: {message}")
            # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ ë” ì¤„ì„
            available = budget_manager.available_tokens - budget_manager.count_tokens(system_prompt)
            user_prompt_with_template = budget_manager._truncate_text(
                user_prompt_with_template,
                available - template.max_tokens
            )
        
        logger.info(
            f"Prompt tokens: {total_tokens} "
            f"(system={budget_manager.count_tokens(system_prompt)}, "
            f"user={budget_manager.count_tokens(user_prompt_with_template)}, "
            f"filtered_rag={len(rag_results)})"
        )
        
        # LLM ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        response_buffer = []
        
        try:
            # ì‹œìŠ¤í…œê³¼ ìœ ì € ë©”ì‹œì§€ ë¶„ë¦¬ (íš¨ìœ¨ì„± 50% í–¥ìƒ)
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt_with_template}
            ]
            
            # generate ë©”ì„œë“œ í˜¸ì¶œ (stream=Trueì¼ ë•Œ AsyncGenerator ë°˜í™˜)
            stream_generator = await self.llm_manager.generate(
                messages,
                stream=True,
                temperature=template.temperature,
                max_tokens=template.max_tokens
            )
            
            # AsyncGenerator ìˆœíšŒ
            async for chunk in stream_generator:
                content = chunk if isinstance(chunk, str) else chunk.get("content", "")
                if content:
                    response_buffer.append(content)
                    
                    yield {
                        "type": ChunkType.RESPONSE.value,
                        "data": {
                            "content": content,
                            "path_source": "web_search",
                            "timestamp": datetime.now().isoformat()
                        }
                    }
            
            # ìµœì¢… ì‘ë‹µ ë©”íƒ€ë°ì´í„°
            full_response: str = "".join(response_buffer)
            
            # ì‘ë‹µ í…œí”Œë¦¿ ê²€ì¦
            validation = template_manager.validate_response(
                response=full_response,
                format_type=ResponseFormat.STANDARD
            )
            
            if not validation["valid"]:
                logger.warning(
                    f"Response validation failed (score={validation['score']:.2f}): "
                    f"{validation['issues']}"
                )
            
            # ì†ŒìŠ¤ ì •ë³´ í¬ë§·íŒ…
            sources: List[Dict[str, Any]] = self._format_sources(rag_results, web_results)
            
            # ë¹„ìš© ì¶”ì •
            input_tokens = budget_manager.count_tokens(system_prompt + user_prompt_with_template)
            output_tokens = budget_manager.count_tokens(full_response)
            estimated_cost = budget_manager.estimate_cost(input_tokens, output_tokens)
            
            # í’ˆì§ˆ í‰ê°€
            from backend.core.quality_evaluator import get_quality_evaluator
            
            quality_evaluator = get_quality_evaluator()
            quality_score = quality_evaluator.evaluate(
                query=query,
                response=full_response,
                sources=sources
            )
            
            logger.info(
                f"Response generated: {output_tokens} tokens, "
                f"estimated cost: ${estimated_cost:.6f}, "
                f"validation_score: {validation['score']:.2f}, "
                f"quality_score: {quality_score.overall:.2f}"
            )
            
            yield {
                "type": ChunkType.FINAL.value,
                "data": {
                    "response": full_response,
                    "sources": sources,
                    "rag_count": len(rag_results),
                    "web_count": len(web_results),
                    "path_source": "web_search",
                    "timestamp": datetime.now().isoformat(),
                    "token_usage": {
                        "input": input_tokens,
                        "output": output_tokens,
                        "total": input_tokens + output_tokens,
                        "estimated_cost": estimated_cost
                    },
                    "quality": {
                        "validation_score": validation["score"],
                        "validation_issues": validation["issues"],
                        "overall_score": quality_score.overall,
                        "relevance": quality_score.relevance,
                        "accuracy": quality_score.accuracy,
                        "completeness": quality_score.completeness,
                        "clarity": quality_score.clarity,
                        "source_citation": quality_score.source_citation,
                        "quality_issues": quality_score.issues
                    },
                    "optimization": {
                        "dynamic_filtering": True,
                        "filtered_rag_count": len(rag_results),
                        "template_format": ResponseFormat.STANDARD.value
                    }
                }
            }
            
        except asyncio.TimeoutError as e:
            logger.error(f"Response generation timeout: {e}", exc_info=True)
            yield {
                "type": ChunkType.ERROR.value,
                "data": {
                    "error": "ì‘ë‹µ ìƒì„± ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.",
                    "error_type": "timeout",
                    "timestamp": datetime.now().isoformat()
                }
            }
        except Exception as e:
            logger.error(f"Response generation failed: {e}", exc_info=True)
            yield {
                "type": ChunkType.ERROR.value,
                "data": {
                    "error": f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}",
                    "error_type": "generation",
                    "timestamp": datetime.now().isoformat()
                }
            }
    
    def _format_sources(
        self,
        rag_results: List[SearchResult],
        web_results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """ì†ŒìŠ¤ ì •ë³´ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        sources: List[Dict[str, Any]] = []
        
        # RAG ì†ŒìŠ¤ (ìƒìœ„ 5ê°œ)
        for result in rag_results[:5]:
            content: str = result.content
            sources.append({
                "type": SourceType.RAG.value,
                "title": result.metadata.get("title", "Internal Document"),
                "content": content[:200] + "..." if len(content) > 200 else content,
                "score": result.score
            })
        
        # Web ì†ŒìŠ¤ (ìƒìœ„ 5ê°œ)
        for result in web_results[:5]:
            snippet: str = result.get("snippet", "")
            sources.append({
                "type": SourceType.WEB.value,
                "title": result.get("title", ""),
                "url": result.get("url", ""),
                "snippet": snippet[:200] + "..." if len(snippet) > 200 else snippet
            })
        
        return sources
    
    def _build_hybrid_prompt(
        self,
        query: str,
        rag_results: List[SearchResult],
        web_results: List[Dict[str, Any]]
    ) -> str:
        """RAG + Web ê²°ê³¼ë¥¼ í†µí•©í•œ í”„ë¡¬í”„íŠ¸ ìƒì„± (ìµœì í™” - í† í° 60% ê°ì†Œ)"""
        
        # í† í° ì˜ˆì‚° ê´€ë¦¬ì ì‚¬ìš©
        from backend.core.token_budget_manager import get_token_budget_manager
        from backend.core.prompt_optimizer import get_prompt_optimizer
        
        budget_manager = get_token_budget_manager(max_tokens=4096)
        prompt_optimizer = get_prompt_optimizer()
        
        # ì˜ˆì‚° í• ë‹¹ (RAG 40%, Web 40%, Query 20%)
        rag_budget, web_budget, query_budget = budget_manager.allocate_budget(
            rag_ratio=0.4,
            web_ratio=0.4,
            query_ratio=0.2
        )
        
        prompt_parts = []
        
        # Query (ì˜ˆì‚° ë‚´)
        query_text = f"Query: {query}"
        if budget_manager.count_tokens(query_text) > query_budget:
            query_text = budget_manager._truncate_text(query_text, query_budget)
        prompt_parts.append(query_text)
        prompt_parts.append("")
        
        # Web ê²°ê³¼ (ìš°ì„ ìˆœìœ„ 1, ì˜ˆì‚° ë‚´)
        if web_results:
            prompt_parts.append("Web Results:")
            web_texts = []
            for i, result in enumerate(web_results[:5], 1):
                title = result.get('title', 'N/A')
                snippet = result.get('snippet', 'N/A')
                web_texts.append(f"{i}. {title}\n{snippet}")
            
            # ì˜ˆì‚°ì— ë§ê²Œ ìë¥´ê¸°
            web_texts_fitted = budget_manager.truncate_to_budget(web_texts, web_budget)
            prompt_parts.extend(web_texts_fitted)
            prompt_parts.append("")
        
        # RAG ê²°ê³¼ (ìš°ì„ ìˆœìœ„ 2, ì˜ˆì‚° ë‚´)
        if rag_results:
            prompt_parts.append("Internal Docs:")
            rag_texts = []
            for i, result in enumerate(rag_results[:5], 1):
                rag_texts.append(f"{i}. {result.content}")
            
            # ì˜ˆì‚°ì— ë§ê²Œ ìë¥´ê¸°
            rag_texts_fitted = budget_manager.truncate_to_budget(rag_texts, rag_budget)
            prompt_parts.extend(rag_texts_fitted)
            prompt_parts.append("")
        
        final_prompt = "\n".join(prompt_parts)
        
        # ì˜ˆì‚° ì²´í¬ ë¡œê¹…
        total_tokens = budget_manager.count_tokens(final_prompt)
        logger.debug(
            f"Hybrid prompt: {total_tokens} tokens "
            f"(web={len(web_results)}, rag={len(rag_results)})"
        )
        
        return final_prompt


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_web_search_agent = None


async def get_web_search_agent() -> WebSearchAgent:
    """Web Search Agent ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _web_search_agent
    
    if _web_search_agent is None:
        from backend.services.embedding import EmbeddingService
        from backend.services.milvus import MilvusManager
        from backend.services.llm_manager import LLMManager
        
        embedding_service = EmbeddingService()
        milvus_manager = MilvusManager(
            host=settings.MILVUS_HOST,
            port=settings.MILVUS_PORT,
            collection_name=settings.MILVUS_COLLECTION_NAME,
            embedding_dim=embedding_service.dimension,
        )
        llm_manager = LLMManager()
        
        _web_search_agent = WebSearchAgent(
            embedding_service=embedding_service,
            milvus_manager=milvus_manager,
            llm_manager=llm_manager
        )
    
    return _web_search_agent
