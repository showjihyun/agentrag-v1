"""
Reranker Performance Benchmark

Comprehensive benchmark comparing:
1. Standard CrossEncoder
2. Optimized Reranker (FP16)
3. Optimized Reranker (INT8)
4. Adaptive Reranker

Tests various scenarios:
- Different document counts (10, 50, 100, 200)
- Different query types (Korean, English, Mixed)
- Cache hit scenarios
- Memory usage
"""

import asyncio
import time
import sys
import psutil
import os
from pathlib import Path
from typing import List, Dict, Any

sys.path.insert(0, str(Path(__file__).parent))


class BenchmarkRunner:
    """Run comprehensive reranker benchmarks"""
    
    def __init__(self):
        self.results = []
        self.process = psutil.Process(os.getpid())
    
    def get_memory_usage(self) -> float:
        """Get current memory usage in MB"""
        return self.process.memory_info().rss / 1024 / 1024
    
    async def benchmark_reranker(
        self,
        name: str,
        reranker: Any,
        query: str,
        results: List[Dict[str, Any]],
        top_k: int = 10,
        warmup: bool = True
    ) -> Dict[str, Any]:
        """Benchmark a single reranker"""
        
        # Warmup run
        if warmup:
            try:
                await reranker.rerank(query, results[:5], top_k=5)
            except:
                pass
        
        # Measure memory before
        mem_before = self.get_memory_usage()
        
        # Benchmark
        times = []
        for i in range(3):  # 3 runs
            start = time.time()
            try:
                reranked = await reranker.rerank(query, results.copy(), top_k=top_k)
                elapsed = (time.time() - start) * 1000
                times.append(elapsed)
            except Exception as e:
                print(f"   âŒ Error: {e}")
                return None
        
        # Measure memory after
        mem_after = self.get_memory_usage()
        
        # Calculate stats
        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)
        
        result = {
            "name": name,
            "avg_time_ms": avg_time,
            "min_time_ms": min_time,
            "max_time_ms": max_time,
            "memory_mb": mem_after - mem_before,
            "num_results": len(reranked) if 'reranked' in locals() else 0,
            "top_score": reranked[0]['score'] if reranked else 0.0
        }
        
        self.results.append(result)
        return result
    
    def print_result(self, result: Dict[str, Any]):
        """Print benchmark result"""
        if result is None:
            return
        
        print(f"   âœ… {result['name']}")
        print(f"      í‰ê· : {result['avg_time_ms']:.2f}ms")
        print(f"      ìµœì†Œ: {result['min_time_ms']:.2f}ms")
        print(f"      ìµœëŒ€: {result['max_time_ms']:.2f}ms")
        print(f"      ë©”ëª¨ë¦¬: {result['memory_mb']:.1f}MB")
        print(f"      Top-1 ì ìˆ˜: {result['top_score']:.4f}")
        print()
    
    def print_comparison(self):
        """Print comparison table"""
        if len(self.results) < 2:
            return
        
        print("=" * 80)
        print("ğŸ“Š ì„±ëŠ¥ ë¹„êµ")
        print("=" * 80)
        print()
        
        # Find baseline (first result)
        baseline = self.results[0]
        
        print(f"{'ëª¨ë¸':<30} {'ì‹œê°„ (ms)':<15} {'ì†ë„ í–¥ìƒ':<15} {'ë©”ëª¨ë¦¬ (MB)':<15}")
        print("-" * 80)
        
        for result in self.results:
            speedup = baseline['avg_time_ms'] / result['avg_time_ms']
            print(
                f"{result['name']:<30} "
                f"{result['avg_time_ms']:>10.2f}ms    "
                f"{speedup:>8.2f}x        "
                f"{result['memory_mb']:>10.1f}MB"
            )
        
        print()


async def benchmark_document_counts():
    """Benchmark different document counts"""
    
    print("=" * 80)
    print("ğŸ“¦ ë¬¸ì„œ ê°œìˆ˜ë³„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    print()
    
    query = "RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ ê°œì„  ë°©ë²•"
    doc_counts = [10, 50, 100, 200]
    
    for count in doc_counts:
        print(f"ğŸ“„ ë¬¸ì„œ {count}ê°œ:")
        print()
        
        # Generate test data
        results = [
            {"text": f"RAG ì‹œìŠ¤í…œ ë¬¸ì„œ {i}: " + "ë‚´ìš© " * 50, "score": 0.8 - i * 0.001}
            for i in range(count)
        ]
        
        runner = BenchmarkRunner()
        
        # Test 1: Standard
        try:
            from backend.services.cross_encoder_reranker import CrossEncoderReranker
            reranker = CrossEncoderReranker("BAAI/bge-reranker-v2-m3")
            result = await runner.benchmark_reranker(
                "Standard CrossEncoder",
                reranker,
                query,
                results
            )
            runner.print_result(result)
        except Exception as e:
            print(f"   âŒ Standard: {e}\n")
        
        # Test 2: Optimized (FP16)
        try:
            from backend.services.optimized_reranker import OptimizedReranker
            reranker = OptimizedReranker(
                "BAAI/bge-reranker-v2-m3",
                use_fp16=True,
                enable_caching=False  # Disable for fair comparison
            )
            result = await runner.benchmark_reranker(
                "Optimized (FP16)",
                reranker,
                query,
                results
            )
            runner.print_result(result)
        except Exception as e:
            print(f"   âŒ Optimized: {e}\n")
        
        # Test 3: Adaptive
        try:
            from backend.services.adaptive_reranker import get_adaptive_reranker
            reranker = get_adaptive_reranker()
            result = await runner.benchmark_reranker(
                "Adaptive Reranker",
                reranker,
                query,
                results
            )
            runner.print_result(result)
        except Exception as e:
            print(f"   âŒ Adaptive: {e}\n")
        
        # Print comparison
        runner.print_comparison()
        print()


async def benchmark_query_types():
    """Benchmark different query types"""
    
    print("=" * 80)
    print("ğŸŒ ì¿¼ë¦¬ íƒ€ì…ë³„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    print()
    
    queries = [
        ("ìˆœìˆ˜ í•œêµ­ì–´", "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì˜ ë°œì „ ë°©í–¥ê³¼ ë¯¸ë˜ ì „ë§"),
        ("í•œì˜ í˜¼í•©", "RAG ì‹œìŠ¤í…œì˜ performance optimization ë°©ë²•"),
        ("ìˆœìˆ˜ ì˜ì–´", "How to improve machine learning model accuracy"),
    ]
    
    # Generate test data
    results = [
        {"text": f"ë¬¸ì„œ {i}: " + "ë‚´ìš© " * 50, "score": 0.8}
        for i in range(50)
    ]
    
    for query_type, query in queries:
        print(f"ğŸ“ {query_type}: {query}")
        print()
        
        runner = BenchmarkRunner()
        
        # Test Adaptive Reranker
        try:
            from backend.services.adaptive_reranker import get_adaptive_reranker
            reranker = get_adaptive_reranker()
            result = await runner.benchmark_reranker(
                "Adaptive Reranker",
                reranker,
                query,
                results
            )
            runner.print_result(result)
            
            # Check which model was used
            if result and 'reranked' in locals():
                print(f"   ì‚¬ìš©ëœ ëª¨ë¸: {reranked[0].get('reranker_model', 'unknown')}")
                print()
        except Exception as e:
            print(f"   âŒ Error: {e}\n")


async def benchmark_cache_performance():
    """Benchmark cache performance"""
    
    print("=" * 80)
    print("ğŸ’¾ ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    print()
    
    query = "RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ ê°œì„ "
    results = [
        {"text": f"ë¬¸ì„œ {i}: " + "ë‚´ìš© " * 50, "score": 0.8}
        for i in range(50)
    ]
    
    try:
        from backend.services.optimized_reranker import OptimizedReranker
        
        reranker = OptimizedReranker(
            "BAAI/bge-reranker-v2-m3",
            use_fp16=True,
            enable_caching=True
        )
        
        # First run (cold cache)
        print("ğŸ”µ ì²« ì‹¤í–‰ (ìºì‹œ ì—†ìŒ):")
        start = time.time()
        reranked1 = await reranker.rerank(query, results.copy(), top_k=10)
        time1 = (time.time() - start) * 1000
        print(f"   ì‹œê°„: {time1:.2f}ms")
        print()
        
        # Second run (warm cache)
        print("ğŸŸ¢ ë‘ ë²ˆì§¸ ì‹¤í–‰ (ìºì‹œ íˆíŠ¸):")
        start = time.time()
        reranked2 = await reranker.rerank(query, results.copy(), top_k=10)
        time2 = (time.time() - start) * 1000
        print(f"   ì‹œê°„: {time2:.2f}ms")
        print()
        
        # Stats
        stats = reranker.get_stats()
        print("ğŸ“Š ìºì‹œ í†µê³„:")
        print(f"   ìºì‹œ íˆíŠ¸ìœ¨: {stats['cache_hit_rate']:.1%}")
        print(f"   ìºì‹œ íˆíŠ¸: {stats['cache_hits']}")
        print(f"   ìºì‹œ ë¯¸ìŠ¤: {stats['cache_misses']}")
        print(f"   ì†ë„ í–¥ìƒ: {time1/time2:.1f}x ğŸš€")
        print()
        
    except Exception as e:
        print(f"âŒ Error: {e}\n")


async def benchmark_memory_usage():
    """Benchmark memory usage"""
    
    print("=" * 80)
    print("ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    print()
    
    query = "í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬"
    results = [{"text": f"ë¬¸ì„œ {i}", "score": 0.8} for i in range(50)]
    
    process = psutil.Process(os.getpid())
    
    models = [
        ("Standard (FP32)", lambda: __import__('backend.services.cross_encoder_reranker', fromlist=['CrossEncoderReranker']).CrossEncoderReranker("BAAI/bge-reranker-v2-m3")),
        ("Optimized (FP16)", lambda: __import__('backend.services.optimized_reranker', fromlist=['OptimizedReranker']).OptimizedReranker("BAAI/bge-reranker-v2-m3", use_fp16=True)),
    ]
    
    for name, create_reranker in models:
        print(f"ğŸ“Š {name}:")
        
        # Measure before
        mem_before = process.memory_info().rss / 1024 / 1024
        
        try:
            reranker = create_reranker()
            await reranker.rerank(query, results[:5], top_k=5)  # Warmup
            
            # Measure after
            mem_after = process.memory_info().rss / 1024 / 1024
            mem_used = mem_after - mem_before
            
            print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©: {mem_used:.1f}MB")
            print()
            
        except Exception as e:
            print(f"   âŒ Error: {e}\n")


async def main():
    """Run all benchmarks"""
    
    print("\n" + "=" * 80)
    print("ğŸš€ Reranker Performance Benchmark")
    print("=" * 80)
    print()
    
    # Benchmark 1: Document counts
    await benchmark_document_counts()
    
    # Benchmark 2: Query types
    await benchmark_query_types()
    
    # Benchmark 3: Cache performance
    await benchmark_cache_performance()
    
    # Benchmark 4: Memory usage
    await benchmark_memory_usage()
    
    print("=" * 80)
    print("âœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")
    print("=" * 80)


if __name__ == "__main__":
    asyncio.run(main())
