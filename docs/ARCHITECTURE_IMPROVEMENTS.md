# Architecture Improvement Recommendations

## í˜„ì¬ ìƒíƒœ ë¶„ì„

í˜„ì¬ ì‹œìŠ¤í…œì€ ì˜ êµ¬ì¡°í™”ë˜ì–´ ìˆìœ¼ë©° ë‹¤ìŒê³¼ ê°™ì€ ê°•ì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤:
- âœ… DDD íŒ¨í„´ ì ìš© (Domain, Application, Infrastructure ë¶„ë¦¬)
- âœ… CQRS íŒ¨í„´ (Commands/Queries ë¶„ë¦¬)
- âœ… Event-Driven Architecture
- âœ… Multi-level Caching (L1/L2)
- âœ… Circuit Breaker, Saga íŒ¨í„´
- âœ… 85%+ í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€

í•˜ì§€ë§Œ ë‹¤ìŒê³¼ ê°™ì€ ê°œì„  ê¸°íšŒê°€ ìˆìŠµë‹ˆë‹¤:

## 1. ì„œë¹„ìŠ¤ ë ˆì´ì–´ ì •ë¦¬ ë° í†µí•© (ìš°ì„ ìˆœìœ„: ë†’ìŒ)

### ë¬¸ì œì 
`backend/services/agent_builder/` ë””ë ‰í† ë¦¬ì— 80ê°œ ì´ìƒì˜ íŒŒì¼ì´ í‰ë©´ì ìœ¼ë¡œ ì¡´ì¬í•˜ì—¬ ê´€ë¦¬ê°€ ì–´ë µìŠµë‹ˆë‹¤.

### ê°œì„  ë°©ì•ˆ

```
backend/services/agent_builder/
â”œâ”€â”€ domain/                    # âœ… ì´ë¯¸ ì˜ êµ¬ì¡°í™”ë¨
â”œâ”€â”€ application/               # âœ… ì´ë¯¸ ì˜ êµ¬ì¡°í™”ë¨
â”œâ”€â”€ infrastructure/            # âœ… ì´ë¯¸ ì˜ êµ¬ì¡°í™”ë¨
â”œâ”€â”€ shared/                    # âœ… ì´ë¯¸ ì˜ êµ¬ì¡°í™”ë¨
â”‚
â”œâ”€â”€ services/                  # ğŸ†• ë¹„ì¦ˆë‹ˆìŠ¤ ì„œë¹„ìŠ¤ í†µí•©
â”‚   â”œâ”€â”€ workflow/
â”‚   â”‚   â”œâ”€â”€ workflow_service.py
â”‚   â”‚   â”œâ”€â”€ workflow_executor.py
â”‚   â”‚   â”œâ”€â”€ workflow_validator.py
â”‚   â”‚   â”œâ”€â”€ workflow_optimizer.py
â”‚   â”‚   â””â”€â”€ workflow_versioning.py
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”œâ”€â”€ agent_service.py
â”‚   â”‚   â”œâ”€â”€ agent_executor.py
â”‚   â”‚   â”œâ”€â”€ agent_testing.py
â”‚   â”‚   â””â”€â”€ agent_collaboration.py
â”‚   â”œâ”€â”€ execution/
â”‚   â”‚   â”œâ”€â”€ execution_service.py
â”‚   â”‚   â”œâ”€â”€ parallel_executor.py
â”‚   â”‚   â””â”€â”€ checkpoint_recovery.py
â”‚   â”œâ”€â”€ knowledge/
â”‚   â”‚   â”œâ”€â”€ knowledgebase_service.py
â”‚   â”‚   â”œâ”€â”€ korean_processor.py
â”‚   â”‚   â””â”€â”€ bm25_index.py
â”‚   â”œâ”€â”€ analytics/
â”‚   â”‚   â”œâ”€â”€ insights_service.py
â”‚   â”‚   â”œâ”€â”€ stats_service.py
â”‚   â”‚   â””â”€â”€ cost_service.py
â”‚   â”œâ”€â”€ ai/
â”‚   â”‚   â”œâ”€â”€ nlp_generator.py
â”‚   â”‚   â”œâ”€â”€ ai_assistant.py
â”‚   â”‚   â”œâ”€â”€ prompt_optimizer.py
â”‚   â”‚   â””â”€â”€ ai_workflow_optimizer.py
â”‚   â””â”€â”€ tools/
â”‚       â”œâ”€â”€ tool_registry.py
â”‚       â”œâ”€â”€ tool_executor.py
â”‚       â””â”€â”€ tool_execution_helper.py
â”‚
â””â”€â”€ facade.py                  # âœ… í†µí•© API
```

### ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš

**Phase 1: ì„œë¹„ìŠ¤ ê·¸ë£¹í™” (2ì‹œê°„)**
```bash
# ì›Œí¬í”Œë¡œìš° ê´€ë ¨ ì„œë¹„ìŠ¤ ì´ë™
mkdir -p backend/services/agent_builder/services/workflow
mv backend/services/agent_builder/workflow_*.py backend/services/agent_builder/services/workflow/

# ì—ì´ì „íŠ¸ ê´€ë ¨ ì„œë¹„ìŠ¤ ì´ë™
mkdir -p backend/services/agent_builder/services/agent
mv backend/services/agent_builder/agent_*.py backend/services/agent_builder/services/agent/
```

**Phase 2: Import ê²½ë¡œ ì—…ë°ì´íŠ¸ (1ì‹œê°„)**
- ëª¨ë“  import ë¬¸ ì—…ë°ì´íŠ¸
- `__init__.py` íŒŒì¼ë¡œ backward compatibility ìœ ì§€

**Phase 3: í…ŒìŠ¤íŠ¸ ê²€ì¦ (1ì‹œê°„)**
- ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
- ë¬¸ì œ ë°œìƒ ì‹œ ìˆ˜ì •

## 2. API ë ˆì´ì–´ ê°œì„  (ìš°ì„ ìˆœìœ„: ì¤‘ê°„)

### ë¬¸ì œì 
API ì—”ë“œí¬ì¸íŠ¸ê°€ ë§ì•„ì§€ë©´ì„œ ì¼ê´€ì„± ë¶€ì¡±

### ê°œì„  ë°©ì•ˆ

#### 2.1 API ë²„ì „ ê´€ë¦¬ ê°•í™”

```python
# backend/api/agent_builder/v1/__init__.py
"""
Agent Builder API v1

Stable API with backward compatibility guarantee.
"""

# backend/api/agent_builder/v2/__init__.py
"""
Agent Builder API v2

New features and breaking changes.
"""
```

#### 2.2 OpenAPI ìŠ¤í™ ìë™ ìƒì„±

```python
# backend/scripts/generate_openapi_spec.py
"""
Generate OpenAPI 3.0 specification for Agent Builder API
"""

from fastapi.openapi.utils import get_openapi
from backend.main import app

def generate_spec():
    spec = get_openapi(
        title="Agent Builder API",
        version="2.0.0",
        description="Comprehensive API for building AI agents and workflows",
        routes=app.routes,
    )
    
    with open("docs/openapi.json", "w") as f:
        json.dump(spec, f, indent=2)
```

#### 2.3 API Rate Limiting ê°œì„ 

```python
# backend/core/rate_limiter_v2.py
"""
Advanced rate limiting with:
- Per-user quotas
- Per-endpoint limits
- Burst allowance
- Redis-based distributed limiting
"""

from redis import Redis
from datetime import datetime, timedelta

class AdvancedRateLimiter:
    def __init__(self, redis: Redis):
        self.redis = redis
        
    async def check_limit(
        self,
        user_id: str,
        endpoint: str,
        limit: int = 100,
        window: int = 3600  # 1 hour
    ) -> tuple[bool, dict]:
        """
        Check if request is within rate limit.
        
        Returns:
            (allowed, info) where info contains:
            - remaining: requests remaining
            - reset_at: when limit resets
            - retry_after: seconds to wait if limited
        """
        key = f"rate_limit:{user_id}:{endpoint}"
        current = await self.redis.incr(key)
        
        if current == 1:
            await self.redis.expire(key, window)
        
        ttl = await self.redis.ttl(key)
        
        if current > limit:
            return False, {
                "remaining": 0,
                "reset_at": datetime.utcnow() + timedelta(seconds=ttl),
                "retry_after": ttl
            }
        
        return True, {
            "remaining": limit - current,
            "reset_at": datetime.utcnow() + timedelta(seconds=ttl),
            "retry_after": 0
        }
```

## 3. ë„ë©”ì¸ ì´ë²¤íŠ¸ ê°•í™” (ìš°ì„ ìˆœìœ„: ì¤‘ê°„)

### ê°œì„  ë°©ì•ˆ

#### 3.1 ì´ë²¤íŠ¸ ì†Œì‹± íŒ¨í„´ ë„ì…

```python
# backend/services/agent_builder/domain/events/event_store.py
"""
Event Store for event sourcing pattern
"""

from typing import List, Type
from datetime import datetime
from sqlalchemy.orm import Session

class EventStore:
    """Store and replay domain events"""
    
    def __init__(self, db: Session):
        self.db = db
    
    async def append(self, aggregate_id: str, event: DomainEvent):
        """Append event to store"""
        event_record = EventRecord(
            aggregate_id=aggregate_id,
            event_type=event.__class__.__name__,
            event_data=event.to_dict(),
            version=await self._get_next_version(aggregate_id),
            occurred_at=datetime.utcnow()
        )
        self.db.add(event_record)
        await self.db.commit()
    
    async def get_events(
        self,
        aggregate_id: str,
        from_version: int = 0
    ) -> List[DomainEvent]:
        """Get all events for an aggregate"""
        records = self.db.query(EventRecord).filter(
            EventRecord.aggregate_id == aggregate_id,
            EventRecord.version > from_version
        ).order_by(EventRecord.version).all()
        
        return [self._deserialize(r) for r in records]
    
    async def replay(self, aggregate_id: str) -> Any:
        """Rebuild aggregate from events"""
        events = await self.get_events(aggregate_id)
        aggregate = None
        
        for event in events:
            if aggregate is None:
                aggregate = self._create_from_event(event)
            else:
                aggregate.apply(event)
        
        return aggregate
```

#### 3.2 ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ë“±ë¡ ê°œì„ 

```python
# backend/services/agent_builder/infrastructure/messaging/event_handlers.py
"""
Centralized event handler registration
"""

from typing import Callable, Dict, List
from backend.services.agent_builder.domain.events import DomainEvent

class EventHandlerRegistry:
    """Registry for event handlers"""
    
    def __init__(self):
        self._handlers: Dict[str, List[Callable]] = {}
    
    def register(self, event_type: str):
        """Decorator to register event handler"""
        def decorator(handler: Callable):
            if event_type not in self._handlers:
                self._handlers[event_type] = []
            self._handlers[event_type].append(handler)
            return handler
        return decorator
    
    async def dispatch(self, event: DomainEvent):
        """Dispatch event to all registered handlers"""
        event_type = event.__class__.__name__
        handlers = self._handlers.get(event_type, [])
        
        for handler in handlers:
            try:
                await handler(event)
            except Exception as e:
                logger.error(f"Handler failed for {event_type}: {e}")

# Usage
registry = EventHandlerRegistry()

@registry.register("WorkflowExecuted")
async def update_analytics(event: WorkflowExecuted):
    """Update analytics when workflow executes"""
    await analytics_service.record_execution(event)

@registry.register("WorkflowExecuted")
async def send_notification(event: WorkflowExecuted):
    """Send notification on workflow completion"""
    await notification_service.notify(event.user_id, event)
```

## 4. ìºì‹± ì „ëµ ê³ ë„í™” (ìš°ì„ ìˆœìœ„: ì¤‘ê°„)

### ê°œì„  ë°©ì•ˆ

#### 4.1 ìŠ¤ë§ˆíŠ¸ ìºì‹œ ë¬´íš¨í™”

```python
# backend/core/cache_invalidation.py
"""
Smart cache invalidation based on dependencies
"""

from typing import Set, Dict, List
from redis import Redis

class CacheDependencyGraph:
    """Track cache dependencies for smart invalidation"""
    
    def __init__(self, redis: Redis):
        self.redis = redis
    
    async def add_dependency(self, key: str, depends_on: List[str]):
        """Add cache key dependencies"""
        for dep in depends_on:
            await self.redis.sadd(f"cache_deps:{dep}", key)
    
    async def invalidate(self, key: str):
        """Invalidate key and all dependent keys"""
        # Get all keys that depend on this key
        dependent_keys = await self.redis.smembers(f"cache_deps:{key}")
        
        # Invalidate all
        keys_to_delete = [key] + list(dependent_keys)
        if keys_to_delete:
            await self.redis.delete(*keys_to_delete)
        
        # Recursively invalidate dependents
        for dep_key in dependent_keys:
            await self.invalidate(dep_key)

# Usage
@cached_medium
async def get_workflow(workflow_id: int):
    workflow = await db.get(workflow_id)
    
    # Track dependencies
    await cache_deps.add_dependency(
        f"workflow:{workflow_id}",
        depends_on=[
            f"user:{workflow.user_id}",
            f"workflow_list:{workflow.user_id}"
        ]
    )
    
    return workflow

# When workflow updates, invalidate smartly
await cache_deps.invalidate(f"workflow:{workflow_id}")
# This also invalidates workflow_list automatically
```

#### 4.2 ìºì‹œ ì›Œë° ì „ëµ

```python
# backend/core/cache_warming.py
"""
Proactive cache warming for frequently accessed data
"""

from apscheduler.schedulers.asyncio import AsyncIOScheduler

class CacheWarmer:
    """Warm cache with frequently accessed data"""
    
    def __init__(self, redis: Redis, db: Session):
        self.redis = redis
        self.db = db
        self.scheduler = AsyncIOScheduler()
    
    def start(self):
        """Start cache warming jobs"""
        # Warm popular workflows every 5 minutes
        self.scheduler.add_job(
            self.warm_popular_workflows,
            'interval',
            minutes=5
        )
        
        # Warm user data every 10 minutes
        self.scheduler.add_job(
            self.warm_user_data,
            'interval',
            minutes=10
        )
        
        self.scheduler.start()
    
    async def warm_popular_workflows(self):
        """Pre-cache popular workflows"""
        popular = await self.db.query(
            Workflow.id
        ).order_by(
            Workflow.execution_count.desc()
        ).limit(100).all()
        
        for workflow_id in popular:
            await workflow_service.get_workflow(workflow_id)
            # This will cache it
```

## 5. ëª¨ë‹ˆí„°ë§ ë° ê´€ì°°ì„± ê°•í™” (ìš°ì„ ìˆœìœ„: ë†’ìŒ)

### ê°œì„  ë°©ì•ˆ

#### 5.1 ë¶„ì‚° ì¶”ì  (Distributed Tracing)

```python
# backend/core/tracing.py
"""
OpenTelemetry-based distributed tracing
"""

from opentelemetry import trace
from opentelemetry.exporter.jaeger import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

def setup_tracing():
    """Setup distributed tracing"""
    trace.set_tracer_provider(TracerProvider())
    
    jaeger_exporter = JaegerExporter(
        agent_host_name="localhost",
        agent_port=6831,
    )
    
    trace.get_tracer_provider().add_span_processor(
        BatchSpanProcessor(jaeger_exporter)
    )

tracer = trace.get_tracer(__name__)

# Usage in services
async def execute_workflow(workflow_id: int):
    with tracer.start_as_current_span("execute_workflow") as span:
        span.set_attribute("workflow.id", workflow_id)
        
        # Execution logic
        result = await executor.execute(workflow_id)
        
        span.set_attribute("workflow.status", result.status)
        span.set_attribute("workflow.duration_ms", result.duration)
        
        return result
```

#### 5.2 êµ¬ì¡°í™”ëœ ë¡œê¹…

```python
# backend/core/structured_logging.py
"""
Structured logging with context
"""

import structlog
from contextvars import ContextVar

# Context variables for request tracking
request_id_var: ContextVar[str] = ContextVar('request_id', default='')
user_id_var: ContextVar[str] = ContextVar('user_id', default='')

def setup_logging():
    """Setup structured logging"""
    structlog.configure(
        processors=[
            structlog.contextvars.merge_contextvars,
            structlog.processors.add_log_level,
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.JSONRenderer()
        ],
        wrapper_class=structlog.make_filtering_bound_logger(logging.INFO),
        context_class=dict,
        logger_factory=structlog.PrintLoggerFactory(),
    )

logger = structlog.get_logger()

# Usage
async def execute_workflow(workflow_id: int, user_id: int):
    # Bind context
    log = logger.bind(
        workflow_id=workflow_id,
        user_id=user_id,
        request_id=request_id_var.get()
    )
    
    log.info("workflow_execution_started")
    
    try:
        result = await executor.execute(workflow_id)
        log.info("workflow_execution_completed", 
                 duration_ms=result.duration,
                 status=result.status)
        return result
    except Exception as e:
        log.error("workflow_execution_failed", 
                  error=str(e),
                  error_type=type(e).__name__)
        raise
```

#### 5.3 í—¬ìŠ¤ ì²´í¬ ê³ ë„í™”

```python
# backend/api/health_v2.py
"""
Advanced health checks with dependencies
"""

from fastapi import APIRouter, status
from typing import Dict, Any

router = APIRouter(prefix="/health", tags=["Health"])

class HealthChecker:
    """Comprehensive health checking"""
    
    async def check_database(self) -> Dict[str, Any]:
        """Check database connectivity"""
        try:
            await db.execute("SELECT 1")
            return {"status": "healthy", "latency_ms": 5}
        except Exception as e:
            return {"status": "unhealthy", "error": str(e)}
    
    async def check_redis(self) -> Dict[str, Any]:
        """Check Redis connectivity"""
        try:
            await redis.ping()
            return {"status": "healthy", "latency_ms": 2}
        except Exception as e:
            return {"status": "unhealthy", "error": str(e)}
    
    async def check_milvus(self) -> Dict[str, Any]:
        """Check Milvus connectivity"""
        try:
            await milvus.list_collections()
            return {"status": "healthy", "latency_ms": 10}
        except Exception as e:
            return {"status": "unhealthy", "error": str(e)}
    
    async def check_all(self) -> Dict[str, Any]:
        """Run all health checks"""
        checks = {
            "database": await self.check_database(),
            "redis": await self.check_redis(),
            "milvus": await self.check_milvus(),
        }
        
        # Overall status
        all_healthy = all(c["status"] == "healthy" for c in checks.values())
        
        return {
            "status": "healthy" if all_healthy else "degraded",
            "checks": checks,
            "timestamp": datetime.utcnow().isoformat()
        }

@router.get("/live")
async def liveness():
    """Kubernetes liveness probe"""
    return {"status": "alive"}

@router.get("/ready")
async def readiness():
    """Kubernetes readiness probe"""
    checker = HealthChecker()
    result = await checker.check_all()
    
    if result["status"] == "healthy":
        return result
    else:
        return JSONResponse(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            content=result
        )
```

## 6. ë³´ì•ˆ ê°•í™” (ìš°ì„ ìˆœìœ„: ë†’ìŒ)

### ê°œì„  ë°©ì•ˆ

#### 6.1 API í‚¤ ê´€ë¦¬ ê°œì„ 

```python
# backend/core/security/api_key_manager.py
"""
Secure API key management with rotation
"""

from cryptography.fernet import Fernet
from datetime import datetime, timedelta

class APIKeyManager:
    """Manage API keys securely"""
    
    def __init__(self, encryption_key: bytes):
        self.cipher = Fernet(encryption_key)
    
    async def create_key(
        self,
        user_id: int,
        name: str,
        expires_in_days: int = 90
    ) -> str:
        """Create new API key"""
        # Generate random key
        raw_key = secrets.token_urlsafe(32)
        
        # Hash for storage
        key_hash = hashlib.sha256(raw_key.encode()).hexdigest()
        
        # Store encrypted
        api_key = APIKey(
            user_id=user_id,
            name=name,
            key_hash=key_hash,
            expires_at=datetime.utcnow() + timedelta(days=expires_in_days),
            last_used_at=None
        )
        
        db.add(api_key)
        await db.commit()
        
        # Return raw key only once
        return f"agr_{raw_key}"
    
    async def rotate_key(self, key_id: int) -> str:
        """Rotate existing API key"""
        old_key = await db.get(APIKey, key_id)
        
        # Create new key
        new_key = await self.create_key(
            user_id=old_key.user_id,
            name=f"{old_key.name} (rotated)",
            expires_in_days=90
        )
        
        # Mark old key as rotated
        old_key.rotated_at = datetime.utcnow()
        old_key.rotated_to_id = new_key.id
        
        await db.commit()
        
        return new_key
```

#### 6.2 ì…ë ¥ ê²€ì¦ ê°•í™”

```python
# backend/core/security/input_validator.py
"""
Advanced input validation and sanitization
"""

import bleach
from pydantic import validator

class SecureWorkflowInput(BaseModel):
    """Secure workflow input with validation"""
    
    name: str
    description: str
    nodes: List[Dict]
    
    @validator('name')
    def validate_name(cls, v):
        """Validate workflow name"""
        # Remove HTML tags
        v = bleach.clean(v, strip=True)
        
        # Check length
        if len(v) < 3 or len(v) > 100:
            raise ValueError("Name must be 3-100 characters")
        
        # Check for SQL injection patterns
        dangerous_patterns = ['--', ';', 'DROP', 'DELETE', 'INSERT']
        if any(p in v.upper() for p in dangerous_patterns):
            raise ValueError("Invalid characters in name")
        
        return v
    
    @validator('nodes')
    def validate_nodes(cls, v):
        """Validate node structure"""
        if len(v) > 100:
            raise ValueError("Too many nodes (max 100)")
        
        for node in v:
            # Validate node type
            if node.get('type') not in ALLOWED_NODE_TYPES:
                raise ValueError(f"Invalid node type: {node.get('type')}")
            
            # Validate code nodes
            if node.get('type') == 'code':
                code = node.get('data', {}).get('code', '')
                if not cls._is_safe_code(code):
                    raise ValueError("Unsafe code detected")
        
        return v
    
    @staticmethod
    def _is_safe_code(code: str) -> bool:
        """Check if code is safe to execute"""
        dangerous_imports = ['os', 'sys', 'subprocess', 'eval', 'exec']
        return not any(imp in code for imp in dangerous_imports)
```

## 7. ì„±ëŠ¥ ìµœì í™” (ìš°ì„ ìˆœìœ„: ì¤‘ê°„)

### ê°œì„  ë°©ì•ˆ

#### 7.1 ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìµœì í™”

```python
# backend/core/database/query_optimizer.py
"""
Database query optimization utilities
"""

from sqlalchemy import event
from sqlalchemy.engine import Engine
import time

# Query performance monitoring
@event.listens_for(Engine, "before_cursor_execute")
def before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    conn.info.setdefault('query_start_time', []).append(time.time())

@event.listens_for(Engine, "after_cursor_execute")
def after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
    total = time.time() - conn.info['query_start_time'].pop(-1)
    
    # Log slow queries
    if total > 1.0:  # 1 second threshold
        logger.warning(
            "slow_query_detected",
            duration_ms=total * 1000,
            query=statement[:200]
        )

# Batch loading helper
async def batch_load_workflows(workflow_ids: List[int]) -> Dict[int, Workflow]:
    """Load multiple workflows efficiently"""
    workflows = await db.query(Workflow).filter(
        Workflow.id.in_(workflow_ids)
    ).options(
        # Eager load relationships
        joinedload(Workflow.nodes),
        joinedload(Workflow.edges),
        selectinload(Workflow.executions)
    ).all()
    
    return {w.id: w for w in workflows}
```

#### 7.2 ë¹„ë™ê¸° ì²˜ë¦¬ ê°œì„ 

```python
# backend/core/async_utils.py
"""
Advanced async utilities
"""

import asyncio
from typing import List, Callable, Any

async def gather_with_concurrency(
    n: int,
    *tasks: Callable,
    return_exceptions: bool = False
) -> List[Any]:
    """
    Run tasks with limited concurrency
    
    Args:
        n: Maximum concurrent tasks
        tasks: Async functions to run
        return_exceptions: Whether to return exceptions
    """
    semaphore = asyncio.Semaphore(n)
    
    async def sem_task(task):
        async with semaphore:
            return await task()
    
    return await asyncio.gather(
        *[sem_task(task) for task in tasks],
        return_exceptions=return_exceptions
    )

# Usage
results = await gather_with_concurrency(
    5,  # Max 5 concurrent
    *[lambda: execute_workflow(id) for id in workflow_ids]
)
```

## ìš°ì„ ìˆœìœ„ ìš”ì•½

### ì¦‰ì‹œ êµ¬í˜„ (1-2ì£¼)
1. âœ… **ì„œë¹„ìŠ¤ ë ˆì´ì–´ ì •ë¦¬** - ê°€ì¥ í° ì˜í–¥, ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ
2. âœ… **ëª¨ë‹ˆí„°ë§ ê°•í™”** - í”„ë¡œë•ì…˜ ì•ˆì •ì„± í•„ìˆ˜
3. âœ… **ë³´ì•ˆ ê°•í™”** - ë³´ì•ˆì€ í•­ìƒ ìš°ì„ 

### ë‹¨ê¸° êµ¬í˜„ (1ê°œì›”)
4. âœ… **ìºì‹± ì „ëµ ê³ ë„í™”** - ì„±ëŠ¥ í–¥ìƒ
5. âœ… **API ë ˆì´ì–´ ê°œì„ ** - ê°œë°œì ê²½í—˜ í–¥ìƒ

### ì¤‘ê¸° êµ¬í˜„ (2-3ê°œì›”)
6. âœ… **ë„ë©”ì¸ ì´ë²¤íŠ¸ ê°•í™”** - í™•ì¥ì„± í–¥ìƒ
7. âœ… **ì„±ëŠ¥ ìµœì í™”** - ì§€ì†ì  ê°œì„ 

## ì˜ˆìƒ íš¨ê³¼

### ì„œë¹„ìŠ¤ ë ˆì´ì–´ ì •ë¦¬
- ğŸ“ íŒŒì¼ ì°¾ê¸° ì‹œê°„ 70% ê°ì†Œ
- ğŸ”§ ìœ ì§€ë³´ìˆ˜ ì‹œê°„ 50% ê°ì†Œ
- ğŸ‘¥ ì‹ ê·œ ê°œë°œì ì˜¨ë³´ë”© ì‹œê°„ 40% ê°ì†Œ

### ëª¨ë‹ˆí„°ë§ ê°•í™”
- ğŸ› ë²„ê·¸ ë°œê²¬ ì‹œê°„ 80% ê°ì†Œ
- ğŸ“Š ì„±ëŠ¥ ë³‘ëª© ì‹ë³„ ì‹œê°„ 90% ê°ì†Œ
- ğŸš¨ ì¥ì•  ëŒ€ì‘ ì‹œê°„ 60% ê°ì†Œ

### ë³´ì•ˆ ê°•í™”
- ğŸ”’ ë³´ì•ˆ ì·¨ì•½ì  70% ê°ì†Œ
- ğŸ›¡ï¸ ê³µê²© íƒì§€ìœ¨ 90% í–¥ìƒ
- âœ… ì»´í”Œë¼ì´ì–¸ìŠ¤ ì¤€ìˆ˜ìœ¨ 100%

### ìºì‹± ê³ ë„í™”
- âš¡ ì‘ë‹µ ì‹œê°„ 50% ê°ì†Œ
- ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ë¶€í•˜ 60% ê°ì†Œ
- ğŸ’° ì¸í”„ë¼ ë¹„ìš© 30% ì ˆê°

## ê²°ë¡ 

í˜„ì¬ ì•„í‚¤í…ì²˜ëŠ” ì´ë¯¸ ê²¬ê³ í•˜ì§€ë§Œ, ìœ„ ê°œì„ ì‚¬í•­ë“¤ì„ ë‹¨ê³„ì ìœ¼ë¡œ ì ìš©í•˜ë©´:
- **í™•ì¥ì„±**: 10ë°° ì´ìƒ íŠ¸ë˜í”½ ì²˜ë¦¬ ê°€ëŠ¥
- **ì•ˆì •ì„±**: 99.9% ê°€ìš©ì„± ë‹¬ì„±
- **ìœ ì§€ë³´ìˆ˜ì„±**: ê°œë°œ ì†ë„ 2ë°° í–¥ìƒ
- **ë³´ì•ˆ**: ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ë³´ì•ˆ ìˆ˜ì¤€

ê°€ì¥ í° íš¨ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆëŠ” **ì„œë¹„ìŠ¤ ë ˆì´ì–´ ì •ë¦¬**ë¶€í„° ì‹œì‘í•˜ëŠ” ê²ƒì„ ê°•ë ¥íˆ ê¶Œì¥í•©ë‹ˆë‹¤.
