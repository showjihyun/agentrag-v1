# ============================================================================
# Agentic RAG System - Environment Configuration
# ============================================================================
# Copy this file to .env and configure the values for your environment
# Required variables are marked with [REQUIRED]
# Optional variables are marked with [OPTIONAL]
# ============================================================================

# ============================================================================
# LLM PROVIDER CONFIGURATION
# ============================================================================
# Primary LLM provider to use for agent reasoning and response generation
# Options: ollama (local), openai (cloud), claude (cloud)
# [REQUIRED]
LLM_PROVIDER=ollama

# Model name to use with the selected provider
# Examples:
#   - Ollama: llama3.1, llama3.2, mistral, mixtral, codellama
#   - OpenAI: gpt-4, gpt-4-turbo, gpt-3.5-turbo
#   - Claude: claude-3-opus-20240229, claude-3-sonnet-20240229, claude-3-haiku-20240307
# [REQUIRED]
LLM_MODEL=gpt-oss

# Fallback providers (comma-separated, optional)
# If primary provider fails, system will try these in order
# Example: LLM_FALLBACK_PROVIDERS=openai,claude
# [OPTIONAL]
LLM_FALLBACK_PROVIDERS=llama3

# ============================================================================
# API KEYS (Cloud Providers)
# ============================================================================
# OpenAI API Key
# Required only if LLM_PROVIDER=openai or openai is in fallback providers
# Get your key from: https://platform.openai.com/api-keys
# [REQUIRED for OpenAI]
OPENAI_API_KEY=

# Anthropic API Key
# Required only if LLM_PROVIDER=claude or claude is in fallback providers
# Get your key from: https://console.anthropic.com/settings/keys
# [REQUIRED for Claude]
ANTHROPIC_API_KEY=

# ============================================================================
# OLLAMA CONFIGURATION (Local LLM)
# ============================================================================
# Ollama base URL for local LLM inference
# Default: http://localhost:11434
# Install Ollama from: https://ollama.ai
# After installation, pull models with: ollama pull llama3.1
# [REQUIRED for Ollama]
OLLAMA_BASE_URL=http://localhost:11434

# ============================================================================
# MILVUS VECTOR DATABASE
# ============================================================================
# Milvus host address
# Use 'localhost' for local development
# Use 'milvus' when running in Docker Compose
# [REQUIRED]
MILVUS_HOST=localhost

# Milvus port
# Default: 19530
# [REQUIRED]
MILVUS_PORT=19530

# Collection name for document embeddings
# [REQUIRED]
MILVUS_COLLECTION_NAME=documents

# Collection name for long-term memory storage
# [REQUIRED]
MILVUS_LTM_COLLECTION_NAME=long_term_memory

# ============================================================================
# REDIS CONFIGURATION (Session & Short-Term Memory)
# ============================================================================
# Redis host address
# Use 'localhost' for local development
# Use 'redis' when running in Docker Compose
# [REQUIRED]
REDIS_HOST=localhost

# Redis port
# Default: 6379
# [REQUIRED]
REDIS_PORT=6379

# Redis database number (0-15)
# [OPTIONAL]
REDIS_DB=0

# Redis password (if authentication is enabled)
# Leave empty if Redis has no password
# [OPTIONAL]
REDIS_PASSWORD=

# ============================================================================
# EMBEDDING MODEL CONFIGURATION
# ============================================================================
# Sentence transformer model for generating embeddings
# 
# Recommended Korean models (in order of quality):
#   1. jhgan/ko-sroberta-multitask (768d, BEST for Korean - specialized Korean model)
#   2. BM-K/KoSimCSE-roberta (768d, excellent for Korean semantic similarity)
#   3. sentence-transformers/paraphrase-multilingual-mpnet-base-v2 (768d, multilingual, good Korean)
#   4. sentence-transformers/distiluse-base-multilingual-cased-v2 (512d, faster, decent Korean)
#
# English only (NOT recommended for Korean):
#   - sentence-transformers/all-MiniLM-L6-v2 (384d, fast, English only)
#   - sentence-transformers/all-mpnet-base-v2 (768d, better quality, English only)
# [REQUIRED]
EMBEDDING_MODEL=jhgan/ko-sroberta-multitask

# ============================================================================
# HYBRID SEARCH CONFIGURATION (Korean-Optimized)
# ============================================================================
# Enable hybrid search (vector + keyword matching)
# Significantly improves Korean search accuracy
# [OPTIONAL - Default: true]
ENABLE_HYBRID_SEARCH=true

# Weight for vector (semantic) search (0.0-1.0)
# Higher values favor semantic similarity
# [OPTIONAL - Default: 0.7]
VECTOR_SEARCH_WEIGHT=0.7

# Weight for keyword (BM25) search (0.0-1.0)
# Higher values favor exact keyword matching
# [OPTIONAL - Default: 0.3]
KEYWORD_SEARCH_WEIGHT=0.3

# ============================================================================
# QUERY EXPANSION CONFIGURATION
# ============================================================================
# Enable query expansion (HyDE, multi-query)
# Improves search accuracy by 12-15% but adds 100-200ms latency
# [OPTIONAL - Default: false]
ENABLE_QUERY_EXPANSION=false

# Query expansion method
# Options: "hyde", "multi", "semantic", "all"
# - hyde: Generate hypothetical answer documents
# - multi: Generate multiple query variations
# - semantic: Add synonyms and related terms
# - all: Use all methods (slowest but most accurate)
# [OPTIONAL - Default: hyde]
QUERY_EXPANSION_METHOD=hyde

# ============================================================================
# RERANKING CONFIGURATION
# ============================================================================
# RERANKING CONFIGURATION
# ============================================================================
# Enable result reranking with cross-encoder
# Improves precision by 25% but adds 50-100ms latency
# Requires downloading cross-encoder model (~100MB)
# [OPTIONAL - Default: false]
ENABLE_RERANKING=false

# Reranking method
# Options: "cross_encoder", "mmr", "hybrid"
# - cross_encoder: Most accurate, slower
# - mmr: Adds diversity, faster
# - hybrid: Both (best quality)
# [OPTIONAL - Default: cross_encoder]
RERANK_METHOD=cross_encoder

# Adaptive Reranking (Auto-selects best model) ⭐ RECOMMENDED
# Automatically chooses between Korean-specialized and multilingual models
# based on content analysis (language, document length, etc.)
# [OPTIONAL - Default: true]
ENABLE_ADAPTIVE_RERANKING=true

# Korean-specialized reranker (for pure Korean content)
# Used when: Korean > 80% AND short documents AND no multilingual
# [OPTIONAL - Default: Dongjin-kr/ko-reranker]
KOREAN_RERANKER_MODEL=Dongjin-kr/ko-reranker

# Multilingual reranker (for mixed/long content)
# Used when: multilingual OR long documents OR English content
# [OPTIONAL - Default: BAAI/bge-reranker-v2-m3]
MULTILINGUAL_RERANKER_MODEL=BAAI/bge-reranker-v2-m3

# Legacy single model (used if adaptive reranking disabled)
# Recommended reranker models (in order of quality):
#   1. BAAI/bge-reranker-v2-m3 (BEST - SOTA multilingual, excellent Korean support) ⭐
#   2. Dongjin-kr/ko-reranker (Korean-specialized cross-encoder)
#   3. BAAI/bge-reranker-v2-gemma (Gemma-based, high quality)
#   4. cross-encoder/ms-marco-MiniLM-L-6-v2 (English-focused, faster)
# [OPTIONAL - Default: BAAI/bge-reranker-v2-m3]
CROSS_ENCODER_MODEL=BAAI/bge-reranker-v2-m3

# ============================================================================
# CACHING CONFIGURATION
# ============================================================================
# Enable search result caching (L1/L2 cache)
# Reduces response time by 50% for repeated queries
# [OPTIONAL - Default: true]
ENABLE_SEARCH_CACHE=true

# L1 cache TTL in seconds (short-term memory)
# [OPTIONAL - Default: 3600 (1 hour)]
CACHE_L1_TTL=3600

# L2 cache promotion threshold (long-term memory)
# Queries searched this many times are promoted to L2
# [OPTIONAL - Default: 3]
CACHE_L2_THRESHOLD=3

# Maximum L2 cache size
# [OPTIONAL - Default: 1000]
CACHE_L2_MAX_SIZE=1000

# ============================================================================
# DOCUMENT PROCESSING CONFIGURATION
# ============================================================================
# Text chunk size in characters
# Recommended: 200-1000 characters
# [OPTIONAL - Default: 500]
CHUNK_SIZE=500

# Chunking strategy for text segmentation
# Options: semantic, sentence, paragraph, heading, fixed
# - semantic: Best quality, groups similar sentences (requires embedding model)
# - sentence: Good balance, respects sentence boundaries
# - paragraph: Fast, respects paragraph boundaries
# - heading: For structured documents with headings
# - fixed: Fallback, fixed-size with sentence boundaries
# [OPTIONAL - Default: semantic]
CHUNKING_STRATEGY=semantic

# Overlap between chunks in characters
# Recommended: 10-20% of chunk size
# [OPTIONAL - Default: 50]
CHUNK_OVERLAP=50

# Maximum file size in bytes
# Default: 52428800 (50MB)
# [OPTIONAL - Default: 50MB]
MAX_FILE_SIZE=52428800

# ============================================================================
# HYBRID DOCUMENT PROCESSING (Native + ColPali)
# ============================================================================
# Enable hybrid processing (Native text + ColPali image embeddings)
# Automatically detects scanned documents and uses appropriate method
# [OPTIONAL - Default: true]
ENABLE_HYBRID_PROCESSING=true

# Text ratio threshold for scanned document detection
# Documents with text_ratio < threshold are treated as scanned
# Range: 0.0 - 1.0, Recommended: 0.3
# [OPTIONAL - Default: 0.3]
HYBRID_COLPALI_THRESHOLD=0.3

# Processing mode:
# - false: Native priority (fast, cost-optimized) - RECOMMENDED for production
#   → Regular documents: Native only (fast)
#   → Scanned documents: ColPali automatically (accurate)
# - true: Hybrid mode (always use both Native + ColPali)
#   → All documents: Native + ColPali (highest accuracy, higher cost)
# [OPTIONAL - Default: false]
HYBRID_PROCESS_IMAGES_ALWAYS=false

# ============================================================================
# MEMORY CONFIGURATION
# ============================================================================
# Short-term memory TTL in seconds
# How long conversation context is retained
# Default: 3600 (1 hour)
# [OPTIONAL - Default: 3600]
STM_TTL=3600

# Maximum number of messages to keep in conversation history
# [OPTIONAL - Default: 20]
MAX_CONVERSATION_HISTORY=20

# ============================================================================
# HYBRID QUERY SYSTEM (SPECULATIVE RAG)
# ============================================================================
# Enable hybrid speculative + agentic query processing
# Provides fast initial responses (~2s) with progressive refinement
# When disabled, system behaves like original Agentic RAG (backward compatible)
# [OPTIONAL - Default: true]
ENABLE_SPECULATIVE_RAG=true

# Timeout for speculative path in seconds
# Fast vector search + simple LLM generation
# Recommended: 2-3 seconds for quick responses
# [OPTIONAL - Default: 2.0]
SPECULATIVE_TIMEOUT=2.0

# Timeout for agentic path in seconds
# Full ReAct/CoT reasoning with agent orchestration
# Recommended: 15-30 seconds depending on query complexity
# [OPTIONAL - Default: 15.0]
AGENTIC_TIMEOUT=15.0

# Default query mode when not specified by user
# Options:
#   - fast: Speculative path only (~2s, basic retrieval)
#   - balanced: Both paths with progressive refinement (~2-5s, best UX)
#   - deep: Agentic path only (~10-15s, comprehensive analysis)
# [OPTIONAL - Default: balanced]
DEFAULT_QUERY_MODE=balanced

# Speculative response cache TTL in seconds
# How long to cache speculative responses for similar queries
# Higher values = more cache hits but potentially stale results
# Recommended: 1800-3600 seconds (30 min - 1 hour)
# [OPTIONAL - Default: 3600]
SPECULATIVE_CACHE_TTL=3600

# Maximum number of cached speculative responses
# Higher values = more memory usage but better cache hit rate
# Recommended: 1000-5000 depending on available memory
# [OPTIONAL - Default: 1000]
SPECULATIVE_CACHE_MAX_SIZE=1000

# ============================================================================
# PERFORMANCE OPTIMIZATION (Phase 1)
# ============================================================================
# LLM timeout for local providers (Ollama) in seconds
# [OPTIONAL - Default: 30]
LLM_TIMEOUT_LOCAL=30

# LLM timeout for cloud providers (OpenAI, Claude) in seconds
# [OPTIONAL - Default: 60]
LLM_TIMEOUT_CLOUD=60

# Keep Milvus collection loaded in memory
# Improves search performance by 50-80%
# [OPTIONAL - Default: true]
MILVUS_KEEP_LOADED=true

# Redis connection pool size
# [OPTIONAL - Default: 50]
REDIS_MAX_CONNECTIONS=50

# Enable fast path for simple queries
# Reduces response time by 60-70% for simple queries
# [OPTIONAL - Default: true]
ENABLE_QUERY_FAST_PATH=true

# Embedding batch size thresholds
# [OPTIONAL - Defaults: 10, 32, 64]
EMBEDDING_BATCH_SIZE_SMALL=10
EMBEDDING_BATCH_SIZE_MEDIUM=32
EMBEDDING_BATCH_SIZE_LARGE=64

# ============================================================================
# PERFORMANCE OPTIMIZATION (Phase 2)
# ============================================================================
# Enable LLM response caching
# Reduces response time by 80% for repeated queries
# [OPTIONAL - Default: true]
ENABLE_LLM_CACHE=true

# LLM cache TTL in seconds
# [OPTIONAL - Default: 3600 (1 hour)]
LLM_CACHE_TTL=3600

# Enable background task processing
# [OPTIONAL - Default: true]
ENABLE_BACKGROUND_TASKS=true

# Maximum concurrent background tasks
# [OPTIONAL - Default: 5]
MAX_CONCURRENT_TASKS=5

# File size threshold for background processing (bytes)
# Files larger than this will be processed in background
# [OPTIONAL - Default: 5242880 (5MB)]
BACKGROUND_UPLOAD_THRESHOLD=5242880

# ============================================================================
# SEMANTIC CACHE CONFIGURATION (Phase 3)
# ============================================================================
# Enable semantic cache with similarity-based retrieval
# Finds cached responses for semantically similar queries
# Reduces response time by 44% on average with cache hits
# [OPTIONAL - Default: true]
ENABLE_SEMANTIC_CACHE=true

# Maximum number of cached entries
# Higher values = more memory usage but better cache hit rate
# Recommended: 1000-5000 depending on available memory
# [OPTIONAL - Default: 1000]
SEMANTIC_CACHE_MAX_SIZE=1000

# Semantic cache TTL in seconds
# How long to keep cached responses
# [OPTIONAL - Default: 3600 (1 hour)]
SEMANTIC_CACHE_TTL=3600

# High similarity threshold (0.0-1.0)
# Queries with similarity >= this value are considered exact matches
# Recommended: 0.95-0.98 for high precision
# [OPTIONAL - Default: 0.95]
SEMANTIC_CACHE_SIMILARITY_HIGH=0.95

# Medium similarity threshold (0.0-1.0)
# Queries with similarity >= this value are considered semantic matches
# Recommended: 0.85-0.90 for balanced precision/recall
# [OPTIONAL - Default: 0.85]
SEMANTIC_CACHE_SIMILARITY_MEDIUM=0.85

# ============================================================================
# FEEDBACK SYSTEM CONFIGURATION (Phase 3)
# ============================================================================
# Enable user feedback collection
# Collects thumbs up/down feedback with detailed categorization
# [OPTIONAL - Default: true]
ENABLE_FEEDBACK_COLLECTION=true

# Feedback storage backend
# Options: "memory" (development), "redis" (production), "database" (future)
# [OPTIONAL - Default: memory]
FEEDBACK_STORAGE_BACKEND=memory

# Feedback retention period in days
# How long to keep feedback data for analysis
# [OPTIONAL - Default: 90]
FEEDBACK_RETENTION_DAYS=90

# ============================================================================
# POSTGRESQL DATABASE (Phase 5)
# ============================================================================
# PostgreSQL connection URL
# Format: postgresql://user:password@host:port/database
# [REQUIRED for Phase 5]
DATABASE_URL=postgresql://raguser:ragpassword@localhost:5433/agentic_rag

# PostgreSQL connection settings
POSTGRES_HOST=localhost
POSTGRES_PORT=5433
POSTGRES_DB=agentic_rag
POSTGRES_USER=raguser
POSTGRES_PASSWORD=ragpassword

# Database pool settings
DB_POOL_SIZE=10
DB_MAX_OVERFLOW=20
DB_POOL_PRE_PING=true

# ============================================================================
# AUTHENTICATION (Phase 5)
# ============================================================================
# JWT secret key for token signing
# Generate with: python -c "import secrets; print(secrets.token_urlsafe(32))"
# [REQUIRED for Phase 5]
JWT_SECRET_KEY=your_jwt_secret_key_here_min_32_chars_change_in_production

# JWT algorithm
JWT_ALGORITHM=HS256

# JWT token expiry in hours
JWT_EXPIRE_HOURS=24

# Refresh token expiry in days
JWT_REFRESH_EXPIRE_DAYS=7

# ============================================================================
# FILE STORAGE (Phase 5)
# ============================================================================
# File storage backend: local | s3 | minio
FILE_STORAGE_BACKEND=local

# Local storage path
LOCAL_STORAGE_PATH=./uploads

# S3/MinIO settings (if using S3 or MinIO)
S3_BUCKET_NAME=rag-documents
S3_ENDPOINT_URL=http://localhost:9000
S3_ACCESS_KEY=minioadmin
S3_SECRET_KEY=minioadmin
S3_REGION=us-east-1

# File upload limits
MAX_FILE_SIZE_MB=50
MAX_BATCH_FILES=100
MAX_BATCH_SIZE_MB=500

# ============================================================================
# HYBRID RAG CONFIGURATION (Static + Agentic Pipeline)
# ============================================================================
# Enable Hybrid RAG system with intelligent routing
# Routes queries to Static RAG (fast) or Agentic RAG (comprehensive) based on complexity
# When disabled, all queries use Agentic RAG (backward compatible)
# [OPTIONAL - Default: true]
HYBRID_RAG_ENABLED=true

# Complexity thresholds for routing decisions (0.0-1.0)
# Queries are analyzed and routed based on complexity score:
#   - Score < SIMPLE: Static RAG (fast, ~1-2s)
#   - Score between SIMPLE and COMPLEX: Static RAG with validation (medium, ~2-3s)
#   - Score > COMPLEX: Agentic RAG (comprehensive, ~10-15s)
# [OPTIONAL - Defaults: 0.3, 0.7]
COMPLEXITY_THRESHOLD_SIMPLE=0.3
COMPLEXITY_THRESHOLD_COMPLEX=0.7

# Confidence thresholds for escalation decisions (0.0-1.0)
# After Static RAG generates a response, confidence is evaluated:
#   - Confidence >= HIGH: Return response immediately (high quality)
#   - Confidence between LOW and HIGH: Return with disclaimer (medium quality)
#   - Confidence < LOW: Escalate to Agentic RAG (low quality, needs refinement)
# [OPTIONAL - Defaults: 0.7, 0.4]
CONFIDENCE_THRESHOLD_HIGH=0.7
CONFIDENCE_THRESHOLD_LOW=0.4

# Static RAG pipeline parameters
# Number of documents to retrieve for static RAG
# [OPTIONAL - Default: 5]
STATIC_RAG_TOP_K=5

# Timeout for static RAG pipeline in seconds
# Target: < 2s for fast responses
# [OPTIONAL - Default: 2.0]
STATIC_RAG_TIMEOUT=2.0

# Enable caching for static RAG responses
# Significantly improves response time for repeated queries
# [OPTIONAL - Default: true]
ENABLE_STATIC_RAG_CACHE=true

# Static RAG cache TTL in seconds
# [OPTIONAL - Default: 3600 (1 hour)]
STATIC_RAG_CACHE_TTL=3600

# Agentic RAG pipeline parameters
# Maximum ReAct iterations for agentic pipeline
# Higher values allow more thorough reasoning but increase latency
# [OPTIONAL - Default: 10]
AGENTIC_RAG_MAX_ITERATIONS=10

# Escalation configuration
# Enable automatic escalation from static to agentic when confidence is low
# [OPTIONAL - Default: true]
ENABLE_AUTO_ESCALATION=true

# Confidence threshold for triggering escalation
# Should match CONFIDENCE_THRESHOLD_LOW for consistency
# [OPTIONAL - Default: 0.4]
ESCALATION_CONFIDENCE_THRESHOLD=0.4

# ============================================================================
# ADAPTIVE ROUTING CONFIGURATION (Intelligent Query-Complexity-Based Routing)
# ============================================================================
# Enable intelligent query-complexity-based routing
# Routes queries to optimal processing mode (FAST/BALANCED/DEEP) based on complexity
# When disabled, uses DEFAULT_QUERY_MODE for all queries
# [OPTIONAL - Default: true]
ADAPTIVE_ROUTING_ENABLED=true

# Adaptive complexity thresholds for intelligent routing (0.0-1.0)
# Queries are analyzed and routed based on complexity score:
#   - Score < SIMPLE: FAST mode (~1s, simple factual queries)
#   - Score between SIMPLE and COMPLEX: BALANCED mode (~3s, multi-faceted queries)
#   - Score > COMPLEX: DEEP mode (~15s, complex analytical queries)
# [OPTIONAL - Defaults: 0.35, 0.70]
ADAPTIVE_COMPLEXITY_THRESHOLD_SIMPLE=0.35
ADAPTIVE_COMPLEXITY_THRESHOLD_COMPLEX=0.70

# Mode timeouts (performance targets in seconds)
# Maximum time allowed for each processing mode
# [OPTIONAL - Defaults: 1.0, 3.0, 15.0]
FAST_MODE_TIMEOUT=1.0
BALANCED_MODE_TIMEOUT=3.0
DEEP_MODE_TIMEOUT=15.0

# Mode parameters (retrieval configuration)
# Number of documents to retrieve for each mode
# [OPTIONAL - Defaults: 5, 10, 15]
FAST_MODE_TOP_K=5
BALANCED_MODE_TOP_K=10
DEEP_MODE_TOP_K=15

# Mode-specific caching configuration (TTL in seconds)
# How long to cache results for each mode
# FAST mode: Higher TTL (more aggressive caching)
# BALANCED mode: Medium TTL
# DEEP mode: Higher TTL (expensive to recompute)
# [OPTIONAL - Defaults: 3600, 1800, 7200]
FAST_MODE_CACHE_TTL=3600
BALANCED_MODE_CACHE_TTL=1800
DEEP_MODE_CACHE_TTL=7200

# Auto-tuning configuration
# Enable automatic threshold adjustment based on performance metrics
# [OPTIONAL - Default: true]
ENABLE_AUTO_THRESHOLD_TUNING=true

# How often to run threshold tuning (in hours)
# [OPTIONAL - Default: 24]
TUNING_INTERVAL_HOURS=24

# Minimum samples required before tuning
# [OPTIONAL - Default: 1000]
TUNING_MIN_SAMPLES=1000

# Run tuning in dry-run mode (no automatic changes)
# Set to false to allow automatic threshold adjustments
# [OPTIONAL - Default: true]
TUNING_DRY_RUN=true

# Pattern learning configuration
# Enable query pattern learning for improved routing
# [OPTIONAL - Default: true]
ENABLE_PATTERN_LEARNING=true

# Minimum samples required to start learning patterns
# [OPTIONAL - Default: 100]
MIN_SAMPLES_FOR_LEARNING=100

# Similarity threshold for pattern matching (0.0-1.0)
# Higher values = stricter matching
# [OPTIONAL - Default: 0.8]
PATTERN_SIMILARITY_THRESHOLD=0.8

# Days of history to consider for pattern learning
# [OPTIONAL - Default: 30]
PATTERN_LEARNING_WINDOW_DAYS=30

# Target mode distribution (for auto-tuning)
# Desired percentage of queries for each mode (0.0-1.0)
# Should sum to approximately 1.0
# [OPTIONAL - Defaults: 0.45, 0.35, 0.20]
TARGET_FAST_MODE_PERCENTAGE=0.45
TARGET_BALANCED_MODE_PERCENTAGE=0.35
TARGET_DEEP_MODE_PERCENTAGE=0.20

# ============================================================================
# APPLICATION CONFIGURATION
# ============================================================================
# Backend API host (for frontend to connect)
# [OPTIONAL - Default: http://localhost:8000]
NEXT_PUBLIC_API_URL=http://localhost:8000

# Enable debug logging
# Set to 'true' for verbose logging
# [OPTIONAL - Default: false]
DEBUG=false

# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
# [OPTIONAL - Default: INFO]
LOG_LEVEL=INFO


# ============================================================================
# IMAGE PROCESSING (ColPali + Fallbacks)
# ============================================================================

# Enable ColPali for image processing (PRIMARY - Best accuracy & speed)
# ColPali provides 95%+ accuracy with fast processing
# [OPTIONAL - Default: true]
ENABLE_COLPALI=true

# Enable Vision LLM fallback (SECONDARY - Requires API key)
# Only used if ColPali fails
# [OPTIONAL - Default: false]
ENABLE_VISION_LLM_FALLBACK=false

# Enable OCR fallback (TERTIARY - Always available)
# Only used if both ColPali and Vision LLM fail
# [OPTIONAL - Default: true]
ENABLE_OCR_FALLBACK=true

# ColPali model name
# [OPTIONAL - Default: vidore/colpali-v1.2]
COLPALI_MODEL=vidore/colpali-v1.2

# Enable binarization (32x memory reduction, 4x speed boost)
# [OPTIONAL - Default: true]
COLPALI_ENABLE_BINARIZATION=true

# Enable pooling (13x speed boost with maintained accuracy)
# [OPTIONAL - Default: true]
COLPALI_ENABLE_POOLING=true

# Pooling factor (9 = 3x3 patch pooling, 1024 → ~128 patches)
# [OPTIONAL - Default: 9]
COLPALI_POOLING_FACTOR=9
